{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install \"torch>=2.0.1\"\n!pip install \"huggingface-hub==0.25.2\" \"transformers==4.41.0\"\n!pip install \"sentence-transformers==2.2.2\"\n!pip install -U langchain-community pinecone-client pipeline\n!pip install \"langchain==0.1.16\" \"langchain-core==0.1.53\" \"langchain-pinecone==0.0.3\" \"pydantic<2\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-26T20:16:43.461081Z","iopub.execute_input":"2025-08-26T20:16:43.461400Z","iopub.status.idle":"2025-08-26T20:17:21.993691Z","shell.execute_reply.started":"2025-08-26T20:16:43.461359Z","shell.execute_reply":"2025-08-26T20:17:21.992813Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.1) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.1) (3.0.2)\nRequirement already satisfied: huggingface-hub==0.25.2 in /usr/local/lib/python3.11/dist-packages (0.25.2)\nCollecting transformers==4.41.0\n  Using cached transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.25.2) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.25.2) (2025.5.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.25.2) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.25.2) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.25.2) (2.32.5)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.25.2) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.25.2) (4.14.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (2024.11.6)\nCollecting tokenizers<0.20,>=0.19 (from transformers==4.41.0)\n  Using cached tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (0.5.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.0) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.25.2) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.25.2) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.25.2) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.25.2) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.41.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.41.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.41.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.41.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.41.0) (2024.2.0)\nUsing cached transformers-4.41.0-py3-none-any.whl (9.1 MB)\nUsing cached tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\nInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.13.3\n    Uninstalling tokenizers-0.13.3:\n      Successfully uninstalled tokenizers-0.13.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.26.1\n    Uninstalling transformers-4.26.1:\n      Successfully uninstalled transformers-4.26.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nspacy-transformers 1.2.1 requires transformers<4.27.0,>=3.4.0, but you have transformers 4.41.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed tokenizers-0.19.1 transformers-4.41.0\nRequirement already satisfied: sentence-transformers==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\nRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (4.41.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (4.67.1)\nRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (0.21.0+cu124)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (1.26.4)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (1.15.3)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (3.9.1)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (0.2.0)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (0.25.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2025.5.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.32.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2024.11.6)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.5.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->sentence-transformers==2.2.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->sentence-transformers==2.2.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->sentence-transformers==2.2.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->sentence-transformers==2.2.2) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->sentence-transformers==2.2.2) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->sentence-transformers==2.2.2) (2.4.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->sentence-transformers==2.2.2) (8.2.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->sentence-transformers==2.2.2) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.6.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->sentence-transformers==2.2.2) (11.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->sentence-transformers==2.2.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->sentence-transformers==2.2.2) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->sentence-transformers==2.2.2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->sentence-transformers==2.2.2) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2025.6.15)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->sentence-transformers==2.2.2) (2024.2.0)\nRequirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.0.38)\nCollecting langchain-community\n  Using cached langchain_community-0.3.28-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: pinecone-client in /usr/local/lib/python3.11/dist-packages (3.2.2)\nCollecting pinecone-client\n  Using cached pinecone_client-6.0.0-py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: pipeline in /usr/local/lib/python3.11/dist-packages (0.1.0)\nCollecting langchain-core<1.0.0,>=0.3.74 (from langchain-community)\n  Using cached langchain_core-0.3.75-py3-none-any.whl.metadata (5.7 kB)\nCollecting langchain<1.0.0,>=0.3.27 (from langchain-community)\n  Using cached langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\nRequirement already satisfied: requests<3,>=2.32.5 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.5)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.13)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\nRequirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.1.147)\nRequirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\nRequirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\nRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2025.6.15)\nRequirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (0.0.7)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.9.0.post0)\nRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (4.14.0)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.5.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\nCollecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain<1.0.0,>=0.3.27->langchain-community)\n  Using cached langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\nCollecting pydantic<3.0.0,>=2.7.4 (from langchain<1.0.0,>=0.3.27->langchain-community)\n  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting langsmith>=0.1.125 (from langchain-community)\n  Downloading langsmith-0.4.18-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-community) (1.33)\nRequirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-community) (23.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\nRequirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\nRequirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\nRequirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2.4.1)\nRequirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.10)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.74->langchain-community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.27->langchain-community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.27->langchain-community) (2.33.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.2->langchain-community) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\nUsing cached langchain_community-0.3.28-py3-none-any.whl (2.5 MB)\nUsing cached pinecone_client-6.0.0-py3-none-any.whl (6.7 kB)\nUsing cached langchain-0.3.27-py3-none-any.whl (1.0 MB)\nUsing cached langchain_core-0.3.75-py3-none-any.whl (443 kB)\nDownloading langsmith-0.4.18-py3-none-any.whl (376 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.4/376.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hUsing cached langchain_text_splitters-0.3.9-py3-none-any.whl (33 kB)\nDownloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pydantic, pinecone-client, langsmith, langchain-core, langchain-text-splitters, langchain, langchain-community\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 1.10.22\n    Uninstalling pydantic-1.10.22:\n      Successfully uninstalled pydantic-1.10.22\n  Attempting uninstall: pinecone-client\n    Found existing installation: pinecone-client 3.2.2\n    Uninstalling pinecone-client-3.2.2:\n      Successfully uninstalled pinecone-client-3.2.2\n  Attempting uninstall: langsmith\n    Found existing installation: langsmith 0.1.147\n    Uninstalling langsmith-0.1.147:\n      Successfully uninstalled langsmith-0.1.147\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.1.53\n    Uninstalling langchain-core-0.1.53:\n      Successfully uninstalled langchain-core-0.1.53\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.0.2\n    Uninstalling langchain-text-splitters-0.0.2:\n      Successfully uninstalled langchain-text-splitters-0.0.2\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.1.16\n    Uninstalling langchain-0.1.16:\n      Successfully uninstalled langchain-0.1.16\n  Attempting uninstall: langchain-community\n    Found existing installation: langchain-community 0.0.38\n    Uninstalling langchain-community-0.0.38:\n      Successfully uninstalled langchain-community-0.0.38\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nspacy 3.4.4 requires pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4, but you have pydantic 2.11.7 which is incompatible.\nscispacy 0.5.5 requires spacy<3.8.0,>=3.7.0, but you have spacy 3.4.4 which is incompatible.\nspacy-transformers 1.2.1 requires transformers<4.27.0,>=3.4.0, but you have transformers 4.41.0 which is incompatible.\nlangchain-pinecone 0.0.3 requires langchain-core<0.2,>=0.1, but you have langchain-core 0.3.75 which is incompatible.\nlangchain-pinecone 0.0.3 requires pinecone-client<4,>=3, but you have pinecone-client 6.0.0 which is incompatible.\ngradio 5.31.0 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.25.2 which is incompatible.\ngradio 5.31.0 requires typer<1.0,>=0.12; sys_platform != \"emscripten\", but you have typer 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.3.27 langchain-community-0.3.28 langchain-core-0.3.75 langchain-text-splitters-0.3.9 langsmith-0.4.18 pinecone-client-6.0.0 pydantic-2.11.7\nCollecting langchain==0.1.16\n  Using cached langchain-0.1.16-py3-none-any.whl.metadata (13 kB)\nCollecting langchain-core==0.1.53\n  Using cached langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: langchain-pinecone==0.0.3 in /usr/local/lib/python3.11/dist-packages (0.0.3)\nCollecting pydantic<2\n  Using cached pydantic-1.10.22-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (154 kB)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (2.0.41)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (3.12.13)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (0.6.7)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (1.33)\nCollecting langchain-community<0.1,>=0.0.32 (from langchain==0.1.16)\n  Using cached langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\nCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain==0.1.16)\n  Using cached langchain_text_splitters-0.0.2-py3-none-any.whl.metadata (2.2 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain==0.1.16)\n  Using cached langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (1.26.4)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (2.32.5)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (8.5.0)\nRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.1.53) (23.2)\nCollecting pinecone-client<4,>=3 (from langchain-pinecone==0.0.3)\n  Using cached pinecone_client-3.2.2-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2) (4.14.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.20.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.16) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.16) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.16) (3.0.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (3.10.18)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (1.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1->langchain==0.1.16) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1->langchain==0.1.16) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1->langchain==0.1.16) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1->langchain==0.1.16) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1->langchain==0.1.16) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1->langchain==0.1.16) (2.4.1)\nRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone-client<4,>=3->langchain-pinecone==0.0.3) (2025.6.15)\nRequirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from pinecone-client<4,>=3->langchain-pinecone==0.0.3) (4.67.1)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client<4,>=3->langchain-pinecone==0.0.3) (2.5.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.1.16) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.1.16) (3.10)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.16) (3.2.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (4.9.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (0.16.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.16) (1.1.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2,>=1->langchain==0.1.16) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2,>=1->langchain==0.1.16) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1->langchain==0.1.16) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2,>=1->langchain==0.1.16) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2,>=1->langchain==0.1.16) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (1.3.1)\nUsing cached langchain-0.1.16-py3-none-any.whl (817 kB)\nUsing cached langchain_core-0.1.53-py3-none-any.whl (303 kB)\nUsing cached pydantic-1.10.22-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\nUsing cached langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\nUsing cached langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\nUsing cached langsmith-0.1.147-py3-none-any.whl (311 kB)\nUsing cached pinecone_client-3.2.2-py3-none-any.whl (215 kB)\nInstalling collected packages: pydantic, pinecone-client, langsmith, langchain-core, langchain-text-splitters, langchain-community, langchain\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.11.7\n    Uninstalling pydantic-2.11.7:\n      Successfully uninstalled pydantic-2.11.7\n  Attempting uninstall: pinecone-client\n    Found existing installation: pinecone-client 6.0.0\n    Uninstalling pinecone-client-6.0.0:\n      Successfully uninstalled pinecone-client-6.0.0\n  Attempting uninstall: langsmith\n    Found existing installation: langsmith 0.4.18\n    Uninstalling langsmith-0.4.18:\n      Successfully uninstalled langsmith-0.4.18\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.75\n    Uninstalling langchain-core-0.3.75:\n      Successfully uninstalled langchain-core-0.3.75\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.3.9\n    Uninstalling langchain-text-splitters-0.3.9:\n      Successfully uninstalled langchain-text-splitters-0.3.9\n  Attempting uninstall: langchain-community\n    Found existing installation: langchain-community 0.3.28\n    Uninstalling langchain-community-0.3.28:\n      Successfully uninstalled langchain-community-0.3.28\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.3.27\n    Uninstalling langchain-0.3.27:\n      Successfully uninstalled langchain-0.3.27\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nscispacy 0.5.5 requires spacy<3.8.0,>=3.7.0, but you have spacy 3.4.4 which is incompatible.\nspacy-transformers 1.2.1 requires transformers<4.27.0,>=3.4.0, but you have transformers 4.41.0 which is incompatible.\npydantic-settings 2.10.1 requires pydantic>=2.7.0, but you have pydantic 1.10.22 which is incompatible.\nsigstore 3.6.4 requires pydantic<3,>=2, but you have pydantic 1.10.22 which is incompatible.\nsigstore-rekor-types 0.0.18 requires pydantic[email]<3,>=2, but you have pydantic 1.10.22 which is incompatible.\nydata-profiling 4.16.1 requires pydantic>=2, but you have pydantic 1.10.22 which is incompatible.\ngoogle-genai 1.21.1 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.22 which is incompatible.\nalbumentations 2.0.8 requires pydantic>=2.9.2, but you have pydantic 1.10.22 which is incompatible.\ngradio 5.31.0 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.25.2 which is incompatible.\ngradio 5.31.0 requires pydantic<2.12,>=2.0, but you have pydantic 1.10.22 which is incompatible.\ngradio 5.31.0 requires typer<1.0,>=0.12; sys_platform != \"emscripten\", but you have typer 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.1.16 langchain-community-0.0.38 langchain-core-0.1.53 langchain-text-splitters-0.0.2 langsmith-0.1.147 pinecone-client-3.2.2 pydantic-1.10.22\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install PyMuPdf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T20:17:21.995637Z","iopub.execute_input":"2025-08-26T20:17:21.995915Z","iopub.status.idle":"2025-08-26T20:17:25.596651Z","shell.execute_reply.started":"2025-08-26T20:17:21.995887Z","shell.execute_reply":"2025-08-26T20:17:25.595804Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: PyMuPdf in /usr/local/lib/python3.11/dist-packages (1.26.4)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install spacy==3.8.0\n!pip install scispacy\n!python -m spacy download en_core_web_sm\n!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_scibert-0.5.1.tar.gz\n!pip install ipywidgets requests pandas tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T20:17:25.597850Z","iopub.execute_input":"2025-08-26T20:17:25.598112Z","iopub.status.idle":"2025-08-26T20:18:14.576372Z","shell.execute_reply.started":"2025-08-26T20:17:25.598083Z","shell.execute_reply":"2025-08-26T20:18:14.575498Z"}},"outputs":[{"name":"stdout","text":"Collecting spacy==3.8.0\n  Using cached spacy-3.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.0) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.0) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.0) (1.0.13)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.0) (2.0.11)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.0) (3.0.10)\nCollecting thinc<8.3.0,>=8.2.2 (from spacy==3.8.0)\n  Using cached thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.0) (0.10.1)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.0) (2.5.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.0) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.0) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.0) (0.7.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.0) (4.67.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.0) (2.32.5)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.0) (1.10.22)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.0) (3.1.6)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.0) (75.2.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.0) (23.2)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.0) (3.5.0)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.0) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.8.0) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy==3.8.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy==3.8.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy==3.8.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy==3.8.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy==3.8.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy==3.8.0) (2.4.1)\nRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.8.0) (4.14.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.8.0) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.8.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.8.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.8.0) (2025.6.15)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy==3.8.0) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy==3.8.0) (0.1.5)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy==3.8.0) (8.2.1)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy==3.8.0) (0.21.1)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy==3.8.0) (6.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy==3.8.0) (3.0.2)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.8.0) (1.2.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.0->spacy==3.8.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.0->spacy==3.8.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.0->spacy==3.8.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.0->spacy==3.8.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.0->spacy==3.8.0) (2024.2.0)\n\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'spacy' candidate (version 3.8.0 at https://files.pythonhosted.org/packages/11/95/12bbc4fa8867d0a616013ce412dfbbec28896284101e6d1c19dc2b7db871/spacy-3.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from https://pypi.org/simple/spacy/) (requires-python:>=3.7))\nReason for being yanked: model compatibility problem\u001b[0m\u001b[33m\n\u001b[0mUsing cached spacy-3.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.4 MB)\nUsing cached thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (920 kB)\nInstalling collected packages: thinc, spacy\n  Attempting uninstall: thinc\n    Found existing installation: thinc 8.1.12\n    Uninstalling thinc-8.1.12:\n      Successfully uninstalled thinc-8.1.12\n  Attempting uninstall: spacy\n    Found existing installation: spacy 3.4.4\n    Uninstalling spacy-3.4.4:\n      Successfully uninstalled spacy-3.4.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nen-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.8.0 which is incompatible.\nen-core-sci-scibert 0.5.1 requires spacy<3.5.0,>=3.4.1, but you have spacy 3.8.0 which is incompatible.\nscispacy 0.5.5 requires spacy<3.8.0,>=3.7.0, but you have spacy 3.8.0 which is incompatible.\nspacy-transformers 1.2.1 requires transformers<4.27.0,>=3.4.0, but you have transformers 4.41.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed spacy-3.8.0 thinc-8.2.5\nRequirement already satisfied: scispacy in /usr/local/lib/python3.11/dist-packages (0.5.5)\nCollecting spacy<3.8.0,>=3.7.0 (from scispacy)\n  Using cached spacy-3.7.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.15.3)\nRequirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scispacy) (2.32.5)\nRequirement already satisfied: conllu in /usr/local/lib/python3.11/dist-packages (from scispacy) (6.0.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.26.4)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.5.1)\nRequirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.2.2)\nRequirement already satisfied: pysbd in /usr/local/lib/python3.11/dist-packages (from scispacy) (0.3.4)\nRequirement already satisfied: nmslib-metabrainz==2.1.3 in /usr/local/lib/python3.11/dist-packages (from scispacy) (2.1.3)\nRequirement already satisfied: pybind11>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from nmslib-metabrainz==2.1.3->scispacy) (2.13.6)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from nmslib-metabrainz==2.1.3->scispacy) (7.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->scispacy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->scispacy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->scispacy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->scispacy) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->scispacy) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->scispacy) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2025.6.15)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.3->scispacy) (3.6.0)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (1.0.13)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.0.11)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.0.10)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (0.10.1)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.5.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (0.7.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (4.67.1)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (1.10.22)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.1.6)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (75.2.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (23.2)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.5.0)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->scispacy) (1.3.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->scispacy) (4.14.0)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->scispacy) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->scispacy) (0.1.5)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->scispacy) (8.2.1)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->scispacy) (0.21.1)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->scispacy) (6.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->scispacy) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->scispacy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->scispacy) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->scispacy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->scispacy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->scispacy) (2024.2.0)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->scispacy) (1.2.1)\nUsing cached spacy-3.7.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\nInstalling collected packages: spacy\n  Attempting uninstall: spacy\n    Found existing installation: spacy 3.8.0\n    Uninstalling spacy-3.8.0:\n      Successfully uninstalled spacy-3.8.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nen-core-sci-scibert 0.5.1 requires spacy<3.5.0,>=3.4.1, but you have spacy 3.7.5 which is incompatible.\nspacy-transformers 1.2.1 requires transformers<4.27.0,>=3.4.0, but you have transformers 4.41.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed spacy-3.7.5\nCollecting en-core-web-sm==3.7.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.13)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.10)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.10.1)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.5)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.22)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.6)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.2.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.1)\nRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.14.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.6.15)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.21.1)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.0)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\nCollecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_scibert-0.5.1.tar.gz\n  Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_scibert-0.5.1.tar.gz (417.6 MB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting spacy<3.5.0,>=3.4.1 (from en_core_sci_scibert==0.5.1)\n  Using cached spacy-3.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\nRequirement already satisfied: spacy-transformers in /usr/local/lib/python3.11/dist-packages (from en_core_sci_scibert==0.5.1) (1.2.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (1.0.13)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2.0.11)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (3.0.10)\nCollecting thinc<8.2.0,>=8.1.0 (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1)\n  Using cached thinc-8.1.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\nRequirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (0.10.1)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2.5.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2.0.10)\nRequirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (0.7.0)\nRequirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (0.11.0)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (4.67.1)\nRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (1.26.4)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2.32.5)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (1.10.22)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (3.1.6)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (75.2.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (23.2)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (3.5.0)\nCollecting transformers<4.27.0,>=3.4.0 (from spacy-transformers->en_core_sci_scibert==0.5.1)\n  Using cached transformers-4.26.1-py3-none-any.whl.metadata (100 kB)\nRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers->en_core_sci_scibert==0.5.1) (2.6.0+cu124)\nRequirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers->en_core_sci_scibert==0.5.1) (0.9.2)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.15.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.15.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.15.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.15.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.15.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.15.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2.4.1)\nRequirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.11/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (0.1.1)\nRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (4.14.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2025.6.15)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (0.1.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (3.5)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from transformers<4.27.0,>=3.4.0->spacy-transformers->en_core_sci_scibert==0.5.1) (0.25.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.27.0,>=3.4.0->spacy-transformers->en_core_sci_scibert==0.5.1) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<4.27.0,>=3.4.0->spacy-transformers->en_core_sci_scibert==0.5.1) (2024.11.6)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<4.27.0,>=3.4.0->spacy-transformers->en_core_sci_scibert==0.5.1)\n  Using cached tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (8.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (3.0.2)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (1.2.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.15.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.15.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.15.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.15.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.15.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2024.2.0)\nUsing cached spacy-3.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\nUsing cached thinc-8.1.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (917 kB)\nUsing cached transformers-4.26.1-py3-none-any.whl (6.3 MB)\nUsing cached tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\nInstalling collected packages: tokenizers, thinc, transformers, spacy\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: thinc\n    Found existing installation: thinc 8.2.5\n    Uninstalling thinc-8.2.5:\n      Successfully uninstalled thinc-8.2.5\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.0\n    Uninstalling transformers-4.41.0:\n      Successfully uninstalled transformers-4.41.0\n  Attempting uninstall: spacy\n    Found existing installation: spacy 3.7.5\n    Uninstalling spacy-3.7.5:\n      Successfully uninstalled spacy-3.7.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nen-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.4.4 which is incompatible.\nscispacy 0.5.5 requires spacy<3.8.0,>=3.7.0, but you have spacy 3.4.4 which is incompatible.\nkaggle-environments 1.17.6 requires transformers>=4.33.1, but you have transformers 4.26.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed spacy-3.4.4 thinc-8.1.12 tokenizers-0.13.3 transformers-4.26.1\nRequirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (8.1.5)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.5)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.2)\nRequirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\nRequirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\nRequirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (4.0.14)\nRequirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.15)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (75.2.0)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\nRequirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\nRequirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.13)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas) (2024.2.0)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install pyngrok\n!pip install fastapi\n!pip install uvicorn\n!pip install requests\n!pip install nest-asyncio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T20:18:14.578518Z","iopub.execute_input":"2025-08-26T20:18:14.578803Z","iopub.status.idle":"2025-08-26T20:18:32.290213Z","shell.execute_reply.started":"2025-08-26T20:18:14.578773Z","shell.execute_reply":"2025-08-26T20:18:32.289388Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.3.0)\nRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\nRequirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.13)\nRequirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.2)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (1.10.22)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.14.0)\nRequirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\nRequirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.3)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.1)\nRequirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.5)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\nRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nimport re\nfrom langchain_pinecone import PineconeVectorStore\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\nfrom transformers import pipeline, AutoModelForCausalLM, AutoTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T20:18:32.291160Z","iopub.execute_input":"2025-08-26T20:18:32.291438Z","iopub.status.idle":"2025-08-26T20:18:41.041005Z","shell.execute_reply.started":"2025-08-26T20:18:32.291407Z","shell.execute_reply":"2025-08-26T20:18:41.040295Z"}},"outputs":[{"name":"stderr","text":"2025-08-26 20:18:33.448668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756239513.475936     294 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756239513.483737     294 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"current_retriever = None\n\n# os.environ[\"PINECONE_API_KEY\"] = PINECONEKEY\n\n# print(\"Pinecone Env Key set successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T20:18:41.042116Z","iopub.execute_input":"2025-08-26T20:18:41.042816Z","iopub.status.idle":"2025-08-26T20:18:41.046744Z","shell.execute_reply.started":"2025-08-26T20:18:41.042784Z","shell.execute_reply":"2025-08-26T20:18:41.045875Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import io\nimport fitz\nimport json\nimport os\nfrom tqdm import tqdm\nfrom PIL import Image\nimport pytesseract\nfrom typing import List\n\nskip_empty_pages = False\nmin_chars_for_page = 1  # page should have at least 1 char\n\n\ndef page_text_from_dict(page):\n    d = page.get_text(\"dict\")  # (blocks, lines, spans)\n    blocks = d.get(\"blocks\", [])\n    text_blocks = []\n    for b in blocks:\n        if b.get(\"type\") != 0:  # Skipping blocks other than text\n            continue\n        bbox = b.get(\"bbox\", [0, 0, 0, 0])  # [x0,y0,x1,y1] -> (left, top, right, bottom)\n        block_lines = []\n        for line in b.get(\"lines\", []):\n            line_text = \" \".join(span.get(\"text\", \"\") for span in line.get(\"spans\", []))\n            if line_text:\n                block_lines.append(line_text)\n        if block_lines:\n            text_blocks.append((bbox[1], bbox[0], \"\\n\".join(block_lines)))  # sort by y, then x\n\n    # sort top->down then left->right\n    text_blocks.sort(key=lambda t: (t[0], t[1]))\n\n    page_text = \"\\n\\n\".join(b[2] for b in text_blocks)\n\n    # Removing Hyphens bw two lines \"exam-\\nple\" -> \"example\"\n    page_text = re.sub(r\"(\\w)-\\n(\\w)\", r\"\\1\\2\", page_text)\n\n    # Replacing excessive newlines with only 2\n    page_text = re.sub(r\"\\n{3,}\", \"\\n\\n\", page_text).strip()\n    return page_text\n\n\ndef ocr_page_with_pytesseract(page, zoom=2):\n    # render at higher resolution for better OCR\n    mat = fitz.Matrix(zoom, zoom)\n    pix = page.get_pixmap(matrix=mat, alpha=False)  # alpha False => no alpha channel\n    mode = \"RGB\" if pix.n >= 3 else \"L\"\n    try:\n        img = Image.frombytes(mode, [pix.width, pix.height], pix.samples)\n    except Exception:\n        # fallback via bytes -> PIL open\n        img = Image.open(io.BytesIO(pix.tobytes()))\n    # run OCR\n    ocr_result = pytesseract.image_to_string(img)\n    return ocr_result\n\n\ndef preprocess_text_pdf(pdf_bytes):\n    doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n    pages_info = []\n    empty_pages = []\n    combined_parts = []\n\n    for pno in tqdm(range(len(doc)), desc=\"Extracting text\"):  # Progress bar\n        page = doc[pno]\n\n        try:\n            text = page_text_from_dict(page) or \"\"\n        except Exception:\n            text = page.get_text(\"text\") or \"\"\n\n        text_stripped = text.rstrip()\n\n        # If page lacks text, attempt OCR on the page image(s)\n        if len(text_stripped) < min_chars_for_page:\n            ocr_text = \"\"\n            try:\n                ocr_text = ocr_page_with_pytesseract(page, zoom=2) or \"\"\n                ocr_text = ocr_text.strip()\n            except Exception:\n                # If OCR/rendering failed, keep empty and log in text snippet\n                ocr_text = \"\"\n\n            if len(ocr_text) >= min_chars_for_page:\n                # OCR found text -> treat page as having text\n                pages_info.append({\"page\": pno + 1, \"text\": ocr_text})\n            else:\n                # truly empty (no text layer and OCR failed or empty)\n                empty_pages.append(pno + 1)\n                pages_info.append({\"page\": pno + 1, \"text\": \"\"})\n            continue\n\n        pages_info.append({\"page\": pno + 1, \"text\": text_stripped})\n\n    print(\"✅ Extraction complete.\")\n    print(f\"Pages processed: {len(doc)}; Empty pages: {empty_pages}\")\n\n    return {\"response\": pages_info}\n\n\ndef preprocess_multiple_pdfs(files: List[tuple]):\n    response_list = []\n    for filename, pdf_bytes in files:\n        # reuse existing function to preserve behavior/format\n        single_result = preprocess_text_pdf(pdf_bytes)  # {\"response\": pages_info}\n        pages_info = single_result.get(\"response\", [])\n        response_list.append({\"filename\": filename, \"page_info\": pages_info})\n    return {\"response\": response_list}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T20:18:41.047628Z","iopub.execute_input":"2025-08-26T20:18:41.047947Z","iopub.status.idle":"2025-08-26T20:18:41.720053Z","shell.execute_reply.started":"2025-08-26T20:18:41.047920Z","shell.execute_reply":"2025-08-26T20:18:41.719460Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# merged_extractor.py\n\"\"\"\nMerged extractor:\n- Input: OCR/text-json (your format with 'response' -> [{'filename', 'page_info': [{'page','text'}]}])\n- NER-first extraction using scispaCy/spaCy and pattern matcher\n- Regex fallback, local-LLM fallback (optional)\n- TOP_K_BY_FEATURE ranking and selection\n- Output schema:\n  { \"file_name\": \"...\", \"name\": \"...\", \"sections\": { \"<HEADING>\": [{\"chunk_text\": \"...\", \"page\": n, \"source\": \"...\"}, ... ] } }\n- every chunk includes \"source\" now.\n\"\"\"\n\nimport re\nimport json\nimport os\nimport traceback\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Tuple, Optional\n\n# try spaCy / scispaCy\ntry:\n    import spacy\n    from spacy.matcher import Matcher\n    try:\n        import scispacy  # presence check only\n    except Exception:\n        scispacy = None\nexcept Exception:\n    spacy = None\n    Matcher = None\n    scispacy = None\n\n# local LLM context (optional)\n_LOCAL_LLM_CTX: Dict[str, Any] = {\"ready\": False, \"tokenizer\": None, \"model\": None, \"name\": None}\n\n# Config\n_HEADINGS = [\n    \"INDICATIONS\", \"INDICATIONS AND USAGE\",\n    \"DOSAGE AND ADMINISTRATION\", \"DOSAGE\", \"ADMINISTRATION\",\n    \"USE IN SPECIFIC POPULATIONS\", \"PREGNANCY\", \"LACTATION\", \"PEDIATRIC USE\", \"GERIATRIC USE\",\n    \"CONTRAINDICATIONS\",\n    \"WARNINGS AND PRECAUTIONS\", \"WARNINGS\",\n    \"ADVERSE REACTIONS\", \"ADVERSE\",\n    \"DRUG INTERACTIONS\", \"DRUG INTERACTION\", \"INTERACTIONS\",\n    \"OVERDOSAGE\", \"OVERDOSE\",\n    \"DESCRIPTION\", \"PRODUCT DESCRIPTION\", \"DESCRIPTION (PRODUCT)\",\n    \"CLINICAL PHARMACOLOGY\", \"PHARMACOLOGY\", \"CLINICAL PHARM\",\n    \"NONCLINICAL TOXICOLOGY\", \"NONCLINICAL\", \"TOXICOLOGY\",\n    \"CLINICAL STUDIES\", \"STUDIES\", \"CLINICAL TRIALS\",\n    \"REFERENCES\", \"BIBLIOGRAPHY\",\n    \"HOW SUPPLIED/STORAGE AND HANDLING\", \"HOW SUPPLIED\", \"STORAGE\", \"HANDLING\",\n    \"PATIENT COUNSELING INFORMATION\", \"PATIENT COUNSELING\", \"PATIENT INFORMATION\",\n]\n\n_ADDITIONAL_HEADINGS = [\n    \"INDICATIONS AND USAGE\",\n    \"DOSAGE AND ADMINISTRATION\",\n    \"CONTRAINDICATIONS\",\n    \"WARNINGS AND PRECAUTIONS\",\n    \"ADVERSE REACTIONS\",\n    \"DRUG INTERACTIONS\",\n    \"USE IN SPECIFIC POPULATIONS\",\n    \"OVERDOSAGE\",\n    \"DESCRIPTION\",\n    \"CLINICAL PHARMACOLOGY\",\n    \"NONCLINICAL TOXICOLOGY\",\n    \"CLINICAL STUDIES\",\n    \"REFERENCES\",\n    \"HOW SUPPLIED/STORAGE AND HANDLING\",\n    \"PATIENT COUNSELING INFORMATION\",\n    \"OTHER\",\n]\n\n_SENT_SPLIT_RE = re.compile(r'(?<=[\\.!?])\\s+')\n_DOSAGE_REGEXES = [\n    re.compile(r\"\\b\\d+(?:[\\.,]\\d+)?\\s?(?:mg|mcg|μg|g)\\b(?:\\s*(?:/|per)\\s*kg)?(?:[^\\n\\.;]{0,120})\", re.I),\n    re.compile(r\"recommended dosage[:\\s\\-]*([^\\n]{0,500})\", re.I),\n]\n_INDICATION_KW = re.compile(r\"\\b(indicat|used to treat|treatment of|indicated for|use for|usage|purpose|intended use)\\b\", re.I)\n_ADVERSE_KW = re.compile(r\"\\b(adverse reaction|adverse event|side effect|side effects)\\b\", re.I)\n_DATE_RE = re.compile(r\"(Revised|Revision|Revision date|Updated|Effective)\\s*[:\\-]?\\s*(\\w+\\s+\\d{1,2},\\s*\\d{4}|\\d{4}-\\d{2}-\\d{2})\", re.I)\n_DRUG_NAME_PATTERN = re.compile(r\"^(?P<brand>[A-Z][A-Z0-9a-z\\-\\s]{1,120})\\s*(?P<bits>[\\d\\w\\.\\s]*)$\", re.M)\n\n# User-provided TOP_K_BY_FEATURE controls\nTOP_K_BY_FEATURE = {\n    \"dosage\": 20,\n    \"indications\": 5,\n    \"warnings\": 10,\n    \"side_effects\": 10,\n    \"contraindications\": 10,\n    \"drug_interactions\": 10,\n    \"overdosage\": 10,\n    \"description\": 10,\n    \"clinical_pharmacology\": 10,\n    \"nonclinical_toxicology\": 10,\n    \"clinical_studies\": 10,\n    \"references\": 2,\n    \"how_supplied\": 2,\n    \"patient_counseling\": 2,\n}\n\n# ------------------- helpers to accept your text-json input -------------------\ndef _pages_from_text_json(text_json: Any) -> Tuple[Optional[List[Dict[str, Any]]], Optional[str]]:\n    \"\"\"\n    Convert OCR/text JSON (your format) into pages list:\n      [{'page': 1, 'text': '...', 'text_lower': '...'}, ...]\n    Accepts dict/list/JSON-string/bytes.\n    \"\"\"\n    if text_json is None:\n        return None, None\n\n    parsed = None\n    if isinstance(text_json, (bytes, bytearray)):\n        try:\n            parsed = json.loads(text_json.decode(\"utf-8\", errors=\"ignore\"))\n        except Exception:\n            try:\n                parsed = json.loads(text_json)\n            except Exception:\n                parsed = None\n    elif isinstance(text_json, str):\n        try:\n            parsed = json.loads(text_json)\n        except Exception:\n            parsed = None\n    elif isinstance(text_json, dict) or isinstance(text_json, list):\n        parsed = text_json\n    else:\n        return None, None\n\n    if parsed is None:\n        return None, None\n\n    # Accept top-level dict with 'response' -> list of file entries\n    resp = None\n    if isinstance(parsed, dict) and \"response\" in parsed and isinstance(parsed[\"response\"], list):\n        resp = parsed[\"response\"]\n    elif isinstance(parsed, list):\n        resp = parsed\n    else:\n        return None, None\n\n    # pick first file entry with page_info if possible\n    file_entry = None\n    if len(resp) == 1:\n        file_entry = resp[0]\n    else:\n        for e in resp:\n            if isinstance(e, dict) and e.get(\"page_info\"):\n                file_entry = e\n                break\n        if file_entry is None:\n            file_entry = resp[0]\n\n    if not isinstance(file_entry, dict):\n        return None, None\n\n    filename = file_entry.get(\"filename\") or file_entry.get(\"name\") or ''\n    page_info = file_entry.get(\"page_info\") or []\n\n    pages: List[Dict[str, Any]] = []\n    for pi in page_info:\n        if not isinstance(pi, dict):\n            continue\n        pnum = pi.get(\"page\")\n        txt = pi.get(\"text\") or \"\"\n        if pnum is None:\n            pnum = len(pages) + 1\n        pages.append({\"page\": int(pnum), \"text\": txt, \"text_lower\": (txt or \"\").lower()})\n    pages = sorted(pages, key=lambda x: int(x.get(\"page\", 0)))\n    return pages, filename\n\n# ------------------- page mapping helpers -------------------\ndef _concat_pages_to_full_text(pages: List[Dict[str,Any]]) -> Tuple[str, List[Tuple[int,int,int]]]:\n    parts = []\n    for p in pages:\n        parts.append(p.get(\"text\",\"\") or \"\")\n    full = \"\\n\".join(parts)\n    ranges = []\n    pos = 0\n    for p in pages:\n        txt = p.get(\"text\",\"\") or \"\"\n        length = len(txt)\n        ranges.append((pos, pos + length, p.get(\"page\")))\n        pos += length + 1\n    return full, ranges\n\ndef _build_page_char_index_map(pages: List[Dict[str,Any]]) -> List[Tuple[int,int,int]]:\n    ranges = []\n    pos = 0\n    for p in pages:\n        txt = p.get(\"text\",\"\") or \"\"\n        length = len(txt)\n        ranges.append((pos, pos + length, p.get(\"page\")))\n        pos += length + 1\n    return ranges\n\ndef _find_page_by_char_index(char_idx: Optional[int], pages: List[Dict[str,Any]], page_map: Optional[List[Tuple[int,int,int]]] = None) -> Optional[int]:\n    if char_idx is None:\n        return None\n    if page_map is None:\n        page_map = _build_page_char_index_map(pages)\n    for s,e,pn in page_map:\n        if s <= char_idx <= e:\n            return pn\n    return None\n\ndef _find_page_for_match(matched_text: str, pages: List[Dict[str, Any]]) -> Optional[int]:\n    if not matched_text:\n        return None\n    needle = re.sub(r\"\\s+\", \" \", matched_text.strip())[:500].lower()\n    if not needle:\n        return None\n    for p in pages:\n        if needle in p.get(\"text_lower\", \"\"):\n            return p[\"page\"]\n    short = needle[:80]\n    for p in pages:\n        if short in p.get(\"text_lower\", \"\"):\n            return p[\"page\"]\n    return None\n\n# ------------------- heading detection (small, robust implementation) -------------------\ndef _find_headings_in_pages(pages: List[Dict[str,Any]]) -> List[Dict[str,Any]]:\n    \"\"\"\n    Returns list of sections detected in the document:\n    [{'section': heading, 'text': section_text, 'start_page': n, 'end_page': m}, ...]\n    Uses _HEADINGS list as canonical anchors; falls back to simple heuristics.\n    \"\"\"\n    if not pages:\n        return []\n    full_text, _ = _concat_pages_to_full_text(pages)\n    sections: List[Dict[str,Any]] = []\n    # build combined heading regex (word-boundary, case-insensitive)\n    heading_patterns = [re.escape(h) for h in _HEADINGS if h and len(h) > 2]\n    if heading_patterns:\n        combined = r'(' + r'|'.join(heading_patterns) + r')'\n        for m in re.finditer(combined, full_text, re.I):\n            try:\n                name = m.group(0).strip()\n                start = m.end()\n                # find next heading occurrence to bound the section\n                next_m = re.search(combined, full_text[start:], re.I)\n                if next_m:\n                    end = start + next_m.start()\n                else:\n                    end = min(len(full_text), start + 8000)\n                snippet = full_text[start:end].strip()\n                if not snippet:\n                    continue\n                start_page = _find_page_for_match(snippet[:400], pages)\n                end_page = _find_page_for_match(snippet[-400:], pages) or start_page\n                sections.append({\"section\": name.upper(), \"text\": snippet, \"start_page\": start_page, \"end_page\": end_page})\n            except Exception:\n                continue\n    # if no sections found, fallback: split pages into pseudo-sections per page\n    if not sections:\n        for p in pages:\n            text = p.get(\"text\",\"\") or \"\"\n            if text.strip():\n                sections.append({\"section\": f\"PAGE {p.get('page')}\", \"text\": text, \"start_page\": p.get(\"page\"), \"end_page\": p.get(\"page\")})\n    return sections\n\n# ------------------- text chunking helpers -------------------\ndef _make_paragraph_from_text_and_index(text: str,\n                                        match_start_idx: Optional[int] = None,\n                                        match_text: Optional[str] = None,\n                                        max_sentences: int = 6,\n                                        max_chars: int = 1600) -> Optional[str]:\n    if not isinstance(text, str) or not text.strip():\n        return None\n    idx = match_start_idx\n    if idx is None and match_text:\n        try:\n            needle = re.sub(r\"\\s+\", \" \", match_text.strip())\n            pos = text.lower().find(needle.lower())\n            if pos >= 0:\n                idx = pos\n        except Exception:\n            idx = None\n    if idx is None:\n        truncated = text.strip()[:max_chars]\n        last_punc = max(truncated.rfind(\".\"), truncated.rfind(\"!\"), truncated.rfind(\"?\"))\n        if last_punc > int(len(truncated) * 0.6):\n            return truncated[: last_punc + 1].strip()\n        return truncated.strip()\n    half = max_chars // 2\n    start = max(0, idx - half)\n    end = min(len(text), idx + half)\n    window = text[start:end].strip()\n    try:\n        sents = _SENT_SPLIT_RE.split(window)\n        if len(sents) > max_sentences:\n            cum = 0\n            target = idx - start\n            sent_idx = 0\n            for i, s in enumerate(sents):\n                cum += len(s) + 1\n                if cum >= target:\n                    sent_idx = i\n                    break\n            left = max(0, sent_idx - (max_sentences // 2))\n            right = min(len(sents), left + max_sentences)\n            sel = \" \".join(sents[left:right]).strip()\n            if len(sel) > max_chars:\n                return sel[:max_chars].strip()\n            return sel\n        else:\n            combined = \" \".join(sents).strip()\n            if len(combined) > max_chars:\n                return combined[:max_chars].strip()\n            return combined\n    except Exception:\n        recommended = window[:max_chars].strip()\n        return recommended\n\n# ------------------- spaCy loader (prioritize scispaCy) -------------------\ndef _safe_load_spacy(model_name: str = \"en_core_sci_scibert\", prefer_scispacy: bool = True):\n    if spacy is None:\n        return None\n    try:\n        candidates = []\n        scispacy_candidates = [\n            \"en_core_sci_scibert\",\n            \"en_core_sci_lg\",\n            \"en_core_sci_md\",\n            \"en_core_sci_sm\",\n        ]\n        fallback_spacy = [\"en_core_web_sm\"]\n        if prefer_scispacy and scispacy is not None:\n            candidates.extend(scispacy_candidates)\n        if model_name and model_name not in candidates:\n            candidates.insert(0, model_name)\n        candidates.extend([c for c in fallback_spacy if c not in candidates])\n\n        nlp = None\n        for cand in candidates:\n            try:\n                nlp = spacy.load(cand)\n                break\n            except Exception:\n                nlp = None\n                continue\n\n        if nlp is None:\n            try:\n                nlp = spacy.load(\"en_core_web_sm\")\n            except Exception:\n                return None\n\n        try:\n            meta_name = (nlp.meta.get(\"name\") or \"\").lower()\n            is_scispacy_model = (\"core_sci\" in meta_name) or (\"scibert\" in meta_name)\n        except Exception:\n            is_scispacy_model = False\n\n        # slim down non-scispacy models but keep ner\n        if not is_scispacy_model:\n            for comp in (\"tagger\", \"parser\"):\n                if comp in nlp.pipe_names:\n                    try:\n                        nlp.remove_pipe(comp)\n                    except Exception:\n                        pass\n        # ensure sentencizer\n        try:\n            if \"sentencizer\" not in nlp.pipe_names:\n                try:\n                    nlp.add_pipe(\"sentencizer\", first=True)\n                except Exception:\n                    from spacy.pipeline import Sentencizer\n                    nlp.add_pipe(\"sentencizer\", first=True)\n        except Exception:\n            pass\n\n        try:\n            nlp.max_length = max(getattr(nlp, \"max_length\", 1000000), 10_000_000)\n        except Exception:\n            pass\n\n        return nlp\n    except Exception:\n        return None\n\n# ------------------- build matcher -------------------\ndef _build_feature_matcher(nlp):\n    if nlp is None:\n        return None\n    m = Matcher(nlp.vocab)\n\n    m.add(\"DOSAGE_MG\", [[{\"LIKE_NUM\": True}, {\"LOWER\": {\"IN\": [\"mg\", \"mcg\", \"μg\", \"g\"]}}]])\n    pop_patterns = [[{\"LOWER\": \"pediatric\"}], [{\"LOWER\": \"children\"}], [{\"LOWER\": \"adult\"}], [{\"LOWER\": \"geriatric\"}], [{\"LOWER\":\"pregnant\"}], [{\"LOWER\":\"pregnancy\"}], [{\"LOWER\":\"breastfeeding\"}]]\n    m.add(\"POPULATION\", pop_patterns)\n\n    ind_patterns = [\n        [{\"LOWER\":\"indicated\"}],\n        [{\"LOWER\":\"indication\"}],\n        [{\"LOWER\":\"indications\"}],\n        [{\"LOWER\":\"used\"},{\"LOWER\":\"to\"},{\"LOWER\":\"treat\"}],\n        [{\"LOWER\":\"treatment\"},{\"LOWER\":\"of\"}],\n        [{\"LOWER\":\"indicated\"},{\"LOWER\":\"for\"}],\n        [{\"LOWER\":\"use\"},{\"LOWER\":\"for\"}],\n        [{\"LOWER\":\"used\"},{\"LOWER\":\"for\"}],\n        [{\"LOWER\":\"usage\"}],\n        [{\"LOWER\":\"purpose\"}],\n        [{\"LOWER\":\"intended\"},{\"LOWER\":\"use\"}],\n        [{\"LOWER\":\"indications\"},{\"LOWER\":\"and\"},{\"LOWER\":\"usage\"}],\n    ]\n    m.add(\"INDICATION\", ind_patterns)\n\n    warn_patterns = [\n        [{\"LOWER\":\"warning\"}],\n        [{\"LOWER\":\"warnings\"}],\n        [{\"LOWER\":\"precaution\"}],\n        [{\"LOWER\":\"precautions\"}],\n        [{\"LOWER\":\"black-box\"}],\n        [{\"LOWER\":\"black\"},{\"LOWER\":\"box\"}],\n        [{\"LOWER\":\"boxed\"},{\"LOWER\":\"warning\"}],\n        [{\"LOWER\":\"try\"},{\"LOWER\":\"avoiding\"}],\n        [{\"LOWER\":\"never\"},{\"LOWER\":\"use\"}],\n        [{\"LOWER\":\"do\"},{\"LOWER\":\"not\"},{\"LOWER\":\"use\"},{\"LOWER\":\"if\"}],\n        [{\"LOWER\":\"do\"},{\"LOWER\":\"not\"},{\"LOWER\":\"use\"}],\n        [{\"LOWER\":\"allergy\"}],\n        [{\"LOWER\":\"allergies\"}],\n    ]\n    m.add(\"WARNING\", warn_patterns)\n\n    adverse_patterns = [\n        [{\"LOWER\":\"adverse\"},{\"LOWER\":\"reaction\"}],\n        [{\"LOWER\":\"adverse\"},{\"LOWER\":\"event\"}],\n        [{\"LOWER\":\"side\"},{\"LOWER\":\"effect\"}],\n        [{\"LOWER\":\"side\"},{\"LOWER\":\"effects\"}],\n        [{\"LOWER\":\"adverse\"}],\n    ]\n    m.add(\"ADVERSE\", adverse_patterns)\n\n    contra_patterns = [\n        [{\"LOWER\":\"contra\"}],\n        [{\"LOWER\":\"contraindication\"}],\n        [{\"LOWER\":\"contraindicated\"}],\n        [{\"LOWER\":\"contraindications\"}],\n        [{\"LOWER\":\"who\"},{\"LOWER\":\"should\"},{\"LOWER\":\"not\"},{\"LOWER\":\"use\"}],\n        [{\"LOWER\":\"do\"},{\"LOWER\":\"not\"},{\"LOWER\":\"recommend\"}],\n        [{\"LOWER\":\"who\"},{\"LOWER\":\"should\"},{\"LOWER\":\"avoid\"}],\n        [{\"LOWER\":\"not\"},{\"LOWER\":\"recommended\"},{\"LOWER\":\"for\"}],\n        [{\"LOWER\":\"should\"},{\"LOWER\":\"not\"},{\"LOWER\":\"be\"},{\"LOWER\":\"used\"}],\n    ]\n    m.add(\"CONTRAINDICATION\", contra_patterns)\n\n    dir_patterns = [\n        [{\"LOWER\":\"direction\"}],\n        [{\"LOWER\":\"directions\"}],\n        [{\"LOWER\":\"directions\"},{\"LOWER\":\"for\"},{\"LOWER\":\"use\"}],\n        [{\"LOWER\":\"administration\"}],\n        [{\"LOWER\":\"how\"},{\"LOWER\":\"to\"},{\"LOWER\":\"use\"}]\n    ]\n    m.add(\"DIRECTION\", dir_patterns)\n\n    drugint_patterns = [\n        [{\"LOWER\":\"drug\"},{\"LOWER\":\"interaction\"}],\n        [{\"LOWER\":\"drug\"},{\"LOWER\":\"interactions\"}],\n        [{\"LOWER\":\"interact\"},{\"LOWER\":\"with\"}],\n        [{\"LOWER\":\"concomitant\"},{\"LOWER\":\"medication\"}],\n        [{\"LOWER\":\"concomitant\"},{\"LOWER\":\"medications\"}],\n        [{\"LOWER\":\"coadministration\"}],\n    ]\n    m.add(\"DRUG_INTERACTION\", drugint_patterns)\n\n    over_patterns = [\n        [{\"LOWER\":\"overdosage\"}],\n        [{\"LOWER\":\"overdose\"}],\n        [{\"LOWER\":\"poisoning\"}],\n        [{\"LOWER\":\"exposure\"},{\"LOWER\":\"to\"},{\"LOWER\":\"an\"}],\n    ]\n    m.add(\"OVERDOSAGE\", over_patterns)\n\n    desc_patterns = [\n        [{\"LOWER\":\"description\"}],\n        [{\"LOWER\":\"description:\"}],\n        [{\"LOWER\":\"chemical\"},{\"LOWER\":\"name\"}],\n        [{\"LOWER\":\"composition\"}],\n        [{\"LOWER\":\"formulation\"}],\n        [{\"LOWER\":\"contains\"}],\n    ]\n    m.add(\"DESCRIPTION\", desc_patterns)\n\n    ph_patterns = [\n        [{\"LOWER\":\"clinical\"},{\"LOWER\":\"pharmacology\"}],\n        [{\"LOWER\":\"mechanism\"},{\"LOWER\":\"of\"},{\"LOWER\":\"action\"}],\n        [{\"LOWER\":\"pharmacokinetics\"}],\n        [{\"LOWER\":\"pharmacodynamics\"}],\n    ]\n    m.add(\"CLINICAL_PHARM\", ph_patterns)\n\n    noncli_patterns = [\n        [{\"LOWER\":\"nonclinical\"},{\"LOWER\":\"toxicology\"}],\n        [{\"LOWER\":\"carcinogenicity\"}],\n        [{\"LOWER\":\"mutagenesis\"}],\n        [{\"LOWER\":\"reproductive\"},{\"LOWER\":\"toxicity\"}],\n    ]\n    m.add(\"NONCLINICAL\", noncli_patterns)\n\n    studies_patterns = [\n        [{\"LOWER\":\"clinical\"},{\"LOWER\":\"studies\"}],\n        [{\"LOWER\":\"clinical\"},{\"LOWER\":\"trial\"}],\n        [{\"LOWER\":\"study\"},{\"LOWER\":\"results\"}],\n    ]\n    m.add(\"CLINICAL_STUDIES\", studies_patterns)\n\n    ref_patterns = [\n        [{\"LOWER\":\"references\"}],\n        [{\"LOWER\":\"bibliography\"}],\n        [{\"LOWER\":\"references:\"}],\n    ]\n    m.add(\"REFERENCES\", ref_patterns)\n\n    supply_patterns = [\n        [{\"LOWER\":\"how\"},{\"LOWER\":\"supplied\"}],\n        [{\"LOWER\":\"how-supplied\"}],\n        [{\"LOWER\":\"storage\"}],\n        [{\"LOWER\":\"storage\"},{\"LOWER\":\"and\"},{\"LOWER\":\"handling\"}],\n        [{\"LOWER\":\"packaging\"}],\n    ]\n    m.add(\"HOW_SUPPLIED\", supply_patterns)\n\n    counsel_patterns = [\n        [{\"LOWER\":\"patient\"},{\"LOWER\":\"counseling\"}],\n        [{\"LOWER\":\"patient\"},{\"LOWER\":\"therapy\"}],\n        [{\"LOWER\":\"counseling\"},{\"LOWER\":\"information\"}],\n    ]\n    m.add(\"PATIENT_COUNSELING\", counsel_patterns)\n\n    return m\n\n# ------------------- Regex search (long windows) -------------------\ndef _regex_search_feature(feature: str, full_text: str, pages: List[Dict[str,Any]], sections: List[Dict[str,Any]]) -> List[Dict[str,Any]]:\n    out = []\n    f = feature.lower()\n    window_left = 600\n    window_right = 900\n\n    def _make_window_from_match(m):\n        start = max(0, m.start() - window_left)\n        end = min(len(full_text), m.end() + window_right)\n        snippet = full_text[start:end]\n        snippet = re.sub(r\"\\s+\", \" \", snippet).strip()\n        page = _find_page_for_match(snippet, pages)\n        para = _make_paragraph_from_text_and_index(full_text, match_start_idx=max(0, m.start()), max_sentences=8, max_chars=2000)\n        return {\"paragraph\": para, \"page\": page, \"match_text\": snippet, \"source\": \"regex\"}\n\n    if f == \"dosage\":\n        for rx in _DOSAGE_REGEXES:\n            for m in rx.finditer(full_text):\n                out.append(_make_window_from_match(m))\n    elif f in (\"indications\", \"indication\"):\n        for m in _INDICATION_KW.finditer(full_text):\n            start = max(0, m.start() - 400)\n            end = min(len(full_text), m.end() + 900)\n            snippet = full_text[start:end]\n            page = _find_page_for_match(snippet, pages)\n            para = _make_paragraph_from_text_and_index(full_text, match_start_idx=m.start(), max_sentences=8, max_chars=2000)\n            out.append({\"paragraph\": para, \"page\": page, \"match_text\": snippet.strip(), \"source\": \"regex\"})\n    elif f in (\"side_effects\",\"adverse\",\"adverse_reactions\"):\n        for m in _ADVERSE_KW.finditer(full_text):\n            start = max(0, m.start() - 400)\n            end = min(len(full_text), m.end() + 900)\n            snippet = full_text[start:end]\n            page = _find_page_for_match(snippet, pages)\n            para = _make_paragraph_from_text_and_index(full_text, match_start_idx=m.start(), max_sentences=8, max_chars=2000)\n            out.append({\"paragraph\": para, \"page\": page, \"match_text\": snippet.strip(), \"source\": \"regex\"})\n    elif f in (\"contraindications\",\"contra\"):\n        for m in re.finditer(r'(Contraindicat(?:ion|ions).{0,800})', full_text, re.I | re.S):\n            snippet = m.group(0)\n            page = _find_page_for_match(snippet, pages)\n            para = _make_paragraph_from_text_and_index(full_text, match_start_idx=max(0, m.start()), max_sentences=8, max_chars=2000)\n            out.append({\"paragraph\": para, \"page\": page, \"match_text\": snippet.strip(), \"source\": \"regex\"})\n    elif f in (\"warnings\",\"warnings_and_precautions\"):\n        for m in re.finditer(r'(Warning[s]?|Precaution[s]?)(.{0,800})', full_text, re.I | re.S):\n            snippet = m.group(0)\n            page = _find_page_for_match(snippet, pages)\n            para = _make_paragraph_from_text_and_index(full_text, match_start_idx=max(0, m.start()), max_sentences=8, max_chars=2000)\n            out.append({\"paragraph\": para, \"page\": page, \"match_text\": snippet.strip(), \"source\": \"regex\"})\n    elif f in (\"drug_interactions\",\"drug_interaction\",\"interactions\"):\n        for m in re.finditer(r'(Drug Interact(?:ion|ions)|interact(?:s|ion)? with|concomitant (?:medication|medications)|coadminister)', full_text, re.I | re.S):\n            out.append(_make_window_from_match(m))\n    elif f in (\"overdosage\",\"overdose\"):\n        for m in re.finditer(r'(Overdosage|Overdose|Poisoning|In case of overdose)(.{0,800})', full_text, re.I | re.S):\n            out.append(_make_window_from_match(m))\n    elif f in (\"clinical_pharmacology\",\"pharmacology\",\"clin_pharm\"):\n        for m in re.finditer(r'(Clinical Pharmacology|Mechanism of Action|Pharmacokinetics|Pharmacodynamics)(.{0,1000})', full_text, re.I | re.S):\n            out.append(_make_window_from_match(m))\n    elif f in (\"nonclinical_toxicology\",\"nonclinical\",\"toxicology\"):\n        for m in re.finditer(r'(Nonclinical Toxicology|Carcinogenicity|Mutagenesis|Fertility)(.{0,1000})', full_text, re.I | re.S):\n            out.append(_make_window_from_match(m))\n    elif f in (\"description\",\"product_description\"):\n        for m in re.finditer(r'(Description|Description:|Formulation|Composition|Chemical name)(.{0,1000})', full_text, re.I | re.S):\n            out.append(_make_window_from_match(m))\n    elif f in (\"clinical_studies\",\"studies\"):\n        for m in re.finditer(r'(Clinical Studies|Study Results|Clinical Trial)(.{0,1000})', full_text, re.I | re.S):\n            out.append(_make_window_from_match(m))\n    elif f in (\"references\",\"bibliography\"):\n        for m in re.finditer(r'(References|Bibliography|References:)(.{0,800})', full_text, re.I | re.S):\n            out.append(_make_window_from_match(m))\n    elif f in (\"how_supplied\",\"how_supplied_storage_and_handling\",\"how supplied\"):\n        for m in re.finditer(r'(How Supplied|How-supplied|Storage and Handling|Storage|Packaging)(.{0,800})', full_text, re.I | re.S):\n            out.append(_make_window_from_match(m))\n    elif f in (\"patient_counseling\",\"patient_information\"):\n        for m in re.finditer(r'(Patient Counseling|Patient Information|Counseling Information)(.{0,800})', full_text, re.I | re.S):\n            out.append(_make_window_from_match(m))\n\n    # dedupe by paragraph text (long windows)\n    seen = set(); uniq=[]\n    for it in out:\n        key = ((it.get(\"paragraph\") or it.get(\"match_text\") or \"\")[:600]).strip().lower()\n        if key and key not in seen:\n            uniq.append(it); seen.add(key)\n    return uniq\n\n# ------------------- dosage parsing/validation -------------------\ndef _parse_and_validate_dosage_text(txt: str) -> Dict[str, Any]:\n    if not txt or not isinstance(txt, str):\n        return {\"amount\": None, \"unit\": None, \"valid\": False, \"reason\": \"empty\"}\n    m = re.search(r\"(\\d+(?:[\\.,]\\d+)?)\\s*(mg|mcg|μg|ug|g)\\b\", txt, re.I)\n    if not m:\n        return {\"amount\": None, \"unit\": None, \"valid\": False, \"reason\": \"no numeric unit\"}\n    raw_amount = m.group(1).replace(\",\", \".\")\n    unit = m.group(2).lower().replace(\"ug\", \"mcg\")\n    try:\n        amt = float(raw_amount)\n    except Exception:\n        return {\"amount\": None, \"unit\": unit, \"valid\": False, \"reason\": \"bad number\"}\n    reason = None\n    valid = True\n    if unit == \"mg\" and amt > 10000:\n        valid = False; reason = \"implausible >10000 mg\"\n    if unit == \"g\" and amt > 1000:\n        valid = False; reason = \"implausible >1000 g\"\n    if unit in (\"mcg\", \"μg\") and amt > 1000000:\n        valid = False; reason = \"implausible micrograms\"\n    if re.search(r\"/\\s?kg|per\\s?kg\", txt, re.I):\n        if unit in (\"mg\",) and amt > 200:\n            valid = False; reason = \"implausible >200 mg/kg\"\n    return {\"amount\": amt, \"unit\": unit, \"valid\": valid, \"reason\": reason}\n\n# ------------------- NER extraction (scispaCy-first, long paragraphs) -------------------\ndef _ner_extract_features(full_text: str, pages: List[Dict[str, Any]], nlp, matcher) -> Dict[str, List[Dict[str, Any]]]:\n    \"\"\"\n    NER-first extraction prioritized for scispaCy models.\n    Returns dict of lists per feature with dicts: {\"paragraph\":..., \"page\":..., \"match_text\":..., \"source\":...}\n    \"\"\"\n    out = {\n        \"indications\": [], \"dosage\": [], \"warnings\": [], \"contraindications\": [], \"side_effects\": [],\n        \"special_populations\": [],\n        \"drug_interactions\": [], \"overdosage\": [], \"description\": [], \"clinical_pharmacology\": [],\n        \"nonclinical_toxicology\": [], \"clinical_studies\": [], \"references\": [], \"how_supplied\": [], \"patient_counseling\": []\n    }\n    if nlp is None:\n        return out\n\n    doc = nlp(full_text)\n    _, full_page_map = _concat_pages_to_full_text(pages)\n\n    model_meta = \"\"\n    try:\n        model_meta = (nlp.meta.get(\"name\") or \"\").lower()\n    except Exception:\n        model_meta = \"\"\n    is_scispacy_model = (\"core_sci\" in model_meta) or (\"scibert\" in model_meta)\n\n    seen_keys = set()\n\n    # 1) doc.ents\n    for ent in getattr(doc, \"ents\", []):\n        ent_text = ent.text.strip()\n        if not ent_text:\n            continue\n        start_char = getattr(ent, \"start_char\", None)\n        para = _make_paragraph_from_text_and_index(full_text, match_start_idx=start_char, match_text=ent_text, max_sentences=8, max_chars=2000)\n        page = _find_page_by_char_index(start_char, pages, page_map=full_page_map)\n        sent_text = ent.sent.text.strip() if hasattr(ent, \"sent\") and ent.sent is not None else ent_text\n        low = sent_text.lower()\n        src_label = \"scispacy_ent\" if is_scispacy_model else \"ner_ent\"\n\n        # Heuristic placements\n        if re.search(r\"\\b\\d+(?:[\\.,]\\d+)?\\s?(mg|mcg|μg|g)\\b\", ent_text, re.I) or re.search(r\"\\b(dose|dosage|tablet|capsule)\\b\", ent_text, re.I):\n            key = (\"dosage_ent\", ent_text.lower()[:350])\n            if key not in seen_keys:\n                out[\"dosage\"].append({\"paragraph\": para, \"page\": page, \"match_text\": ent_text, \"source\": src_label})\n                seen_keys.add(key)\n            continue\n        if any(k in low for k in (\"indicat\", \"used to treat\", \"indicated for\", \"treatment of\", \"use for\", \"used for\", \"intended use\")):\n            key = (\"ind_ent\", ent_text.lower()[:300])\n            if key not in seen_keys:\n                out[\"indications\"].append({\"paragraph\": para, \"page\": page, \"match_text\": ent_text, \"source\": src_label})\n                seen_keys.add(key)\n            continue\n        if any(k in low for k in (\"adverse\", \"side effect\", \"side effects\", \"adverse effects\")) or any(k in ent_text.lower() for k in (\"rash\",\"nausea\",\"vomit\",\"diarrh\",\"dizziness\",\"headache\")):\n            key = (\"adverse_ent\", ent_text.lower()[:300])\n            if key not in seen_keys:\n                out[\"side_effects\"].append({\"paragraph\": para, \"page\": page, \"match_text\": ent_text, \"source\": src_label})\n                seen_keys.add(key)\n            continue\n        if any(k in low for k in (\"contraindicat\", \"do not use\", \"who should avoid\", \"should not be used\")):\n            key = (\"contra_ent\", ent_text.lower()[:300])\n            if key not in seen_keys:\n                out[\"contraindications\"].append({\"paragraph\": para, \"page\": page, \"match_text\": ent_text, \"source\": src_label})\n                seen_keys.add(key)\n            continue\n        if any(k in low for k in (\"interact\", \"interaction\", \"concomitant\", \"coadminister\")):\n            key = (\"drugint_ent\", ent_text.lower()[:300])\n            if key not in seen_keys:\n                out[\"drug_interactions\"].append({\"paragraph\": para, \"page\": page, \"match_text\": ent_text, \"source\": src_label})\n                seen_keys.add(key)\n            continue\n        if any(k in ent_text.lower() for k in (\"acid\", \"salt\", \"hydrate\", \"sodium\", \"chloride\", \"methyl\", \"phenyl\", \"derivative\")):\n            key = (\"desc_ent\", ent_text.lower()[:300])\n            if key not in seen_keys:\n                out[\"description\"].append({\"paragraph\": para, \"page\": page, \"match_text\": ent_text, \"source\": src_label})\n                seen_keys.add(key)\n            continue\n        # fallback: description candidate\n        key = (\"other_ent\", ent_text.lower()[:300])\n        if key not in seen_keys:\n            out[\"description\"].append({\"paragraph\": para, \"page\": page, \"match_text\": ent_text, \"source\": src_label})\n            seen_keys.add(key)\n\n    # 2) matcher patterns\n    if matcher is not None:\n        matches = matcher(doc)\n        for mid, start, end in matches:\n            label = nlp.vocab.strings[mid]\n            span = doc[start:end]\n            span_text = span.text.strip()\n            if not span_text:\n                continue\n            start_char = getattr(span, \"start_char\", None)\n            para = _make_paragraph_from_text_and_index(full_text, match_start_idx=start_char, match_text=span_text, max_sentences=8, max_chars=2000)\n            page = _find_page_by_char_index(start_char, pages, page_map=full_page_map)\n            src_label = \"scispacy\" if is_scispacy_model else \"ner\"\n            entry = {\"paragraph\": para, \"page\": page, \"match_text\": span_text, \"source\": src_label}\n\n            if label == \"INDICATION\":\n                key = (\"indication\", span_text.lower()[:160])\n                if key not in seen_keys:\n                    out[\"indications\"].append(entry); seen_keys.add(key)\n            elif label == \"DOSAGE_MG\":\n                key = (\"dosage\", span_text.lower()[:350])\n                if key not in seen_keys:\n                    out[\"dosage\"].append(entry); seen_keys.add(key)\n            elif label == \"POPULATION\":\n                pop = span_text.lower()\n                out[\"special_populations\"].append({\"population\": pop, \"paragraph\": para, \"page\": page, \"source\": src_label})\n            elif label == \"WARNING\":\n                key = (\"warning\", span_text.lower()[:160])\n                if key not in seen_keys:\n                    out[\"warnings\"].append(entry); seen_keys.add(key)\n            elif label == \"ADVERSE\":\n                key = (\"adverse\", span_text.lower()[:160])\n                if key not in seen_keys:\n                    out[\"side_effects\"].append(entry); seen_keys.add(key)\n            elif label == \"CONTRAINDICATION\":\n                key = (\"contra\", span_text.lower()[:320])\n                if key not in seen_keys:\n                    out[\"contraindications\"].append(entry); seen_keys.add(key)\n            elif label == \"DRUG_INTERACTION\":\n                key = (\"drug_interaction\", span_text.lower()[:160])\n                if key not in seen_keys:\n                    out[\"drug_interactions\"].append(entry); seen_keys.add(key)\n            elif label == \"OVERDOSAGE\":\n                key = (\"overdosage\", span_text.lower()[:160])\n                if key not in seen_keys:\n                    out[\"overdosage\"].append(entry); seen_keys.add(key)\n            elif label == \"DESCRIPTION\":\n                key = (\"description\", span_text.lower()[:160])\n                if key not in seen_keys:\n                    out[\"description\"].append(entry); seen_keys.add(key)\n            elif label == \"CLINICAL_PHARM\":\n                key = (\"clinical_pharmacology\", span_text.lower()[:320])\n                if key not in seen_keys:\n                    out[\"clinical_pharmacology\"].append(entry); seen_keys.add(key)\n            elif label == \"NONCLINICAL\":\n                key = (\"nonclinical_toxicology\", span_text.lower()[:320])\n                if key not in seen_keys:\n                    out[\"nonclinical_toxicology\"].append(entry); seen_keys.add(key)\n            elif label == \"CLINICAL_STUDIES\":\n                key = (\"clinical_studies\", span_text.lower()[:160])\n                if key not in seen_keys:\n                    out[\"clinical_studies\"].append(entry); seen_keys.add(key)\n            elif label == \"REFERENCES\":\n                key = (\"references\", span_text.lower()[:160])\n                if key not in seen_keys:\n                    out[\"references\"].append(entry); seen_keys.add(key)\n            elif label == \"HOW_SUPPLIED\":\n                key = (\"how_supplied\", span_text.lower()[:160])\n                if key not in seen_keys:\n                    out[\"how_supplied\"].append(entry); seen_keys.add(key)\n            elif label == \"PATIENT_COUNSELING\":\n                key = (\"patient_counseling\", span_text.lower()[:160])\n                if key not in seen_keys:\n                    out[\"patient_counseling\"].append(entry); seen_keys.add(key)\n\n    # 3) sentence-level heuristics\n    for sent in doc.sents:\n        s_text = sent.text.strip()\n        if not s_text:\n            continue\n        start_char = getattr(sent, \"start_char\", None)\n        para = _make_paragraph_from_text_and_index(full_text, match_start_idx=start_char, match_text=s_text, max_sentences=8, max_chars=2000)\n        page = _find_page_by_char_index(start_char, pages, page_map=full_page_map)\n        low = s_text.lower()\n        src_label = \"scispacy\" if is_scispacy_model else \"ner_sent\"\n\n        if any(k in low for k in (\"indicat\", \"used to treat\", \"indicated for\", \"treatment of\", \"use for\", \"used for\", \"usage\", \"purpose\", \"intended use\")):\n            key = (\"indication_sent\", low[:160])\n            if key not in seen_keys:\n                out[\"indications\"].append({\"paragraph\": para, \"page\": page, \"match_text\": s_text, \"source\": src_label}); seen_keys.add(key)\n        if any(k in low for k in (\"contraindicat\", \"do not use\", \"who should avoid\", \"should not be used\")):\n            key = (\"contra_sent\", low[:340])\n            if key not in seen_keys:\n                out[\"contraindications\"].append({\"paragraph\": para, \"page\": page, \"match_text\": s_text, \"source\": src_label}); seen_keys.add(key)\n        if any(k in low for k in (\"adverse\", \"side effect\", \"side effects\", \"adverse effects\")):\n            key = (\"adverse_sent\", low[:320])\n            if key not in seen_keys:\n                out[\"side_effects\"].append({\"paragraph\": para, \"page\": page, \"match_text\": s_text, \"source\": src_label}); seen_keys.add(key)\n        if any(k in low for k in (\"warning\",'allergies','allergens',\"precaution\",\"allergy alert\",\"black box\",\"boxed warning\")):\n            key = (\"warn_sent\", low[:360])\n            if key not in seen_keys:\n                out[\"warnings\"].append({\"paragraph\": para, \"page\": page, \"match_text\": s_text, \"source\": src_label}); seen_keys.add(key)\n        if any(k in low for k in ('direc', \"recommended dosage\", \"directions\",'dose','dosage','directions for use','suggested dosage')):\n            key = (\"dosage_sent\", low[:800])\n            if key not in seen_keys:\n                out[\"dosage\"].append({\"paragraph\": para, \"page\": page, \"match_text\": s_text, \"source\": src_label}); seen_keys.add(key)\n\n        if any(k in low for k in (\"drug interaction\",\"interact\", \"interaction\", \"concomitant\", \"coadministration\")):\n            key = (\"drugint_sent\", low[:320])\n            if key not in seen_keys:\n                out[\"drug_interactions\"].append({\"paragraph\": para, \"page\": page, \"match_text\": s_text, \"source\": src_label}); seen_keys.add(key)\n        if any(k in low for k in (\"overdose\", \"overdosage\", \"poisoning\", \"exposure to\")):\n            key = (\"overdose_sent\", low[:320])\n            if key not in seen_keys:\n                out[\"overdosage\"].append({\"paragraph\": para, \"page\": page, \"match_text\": s_text, \"source\": src_label}); seen_keys.add(key)\n        if any(k in low for k in (\"clinical pharmacology\",\"pharmacokinetic\", \"pharmacodynamics\", \"mechanism of action\", \"pharmacology\")):\n            key = (\"pharm_sent\", low[:320])\n            if key not in seen_keys:\n                out[\"clinical_pharmacology\"].append({\"paragraph\": para, \"page\": page, \"match_text\": s_text, \"source\": src_label}); seen_keys.add(key)\n        if any(k in low for k in (\"nonclinical\", \"toxicology\",\"nonclinical toxicology\", \"carcinogenicity\", \"mutagenesis\", \"fertility\")):\n            key = (\"noncl_sent\", low[:320])\n            if key not in seen_keys:\n                out[\"nonclinical_toxicology\"].append({\"paragraph\": para, \"page\": page, \"match_text\": s_text, \"source\": src_label}); seen_keys.add(key)\n        if any(k in low for k in (\"clinical study\", \"study results\", \"clinical studies\", \"trial\")):\n            key = (\"clinstudy_sent\", low[:320])\n            if key not in seen_keys:\n                out[\"clinical_studies\"].append({\"paragraph\": para, \"page\": page, \"match_text\": s_text, \"source\": src_label}); seen_keys.add(key)\n        if any(k in low for k in (\"references\", \"bibliography\", \"reference\")):\n            key = (\"ref_sent\", low[:320])\n            if key not in seen_keys:\n                out[\"references\"].append({\"paragraph\": para, \"page\": page, \"match_text\": s_text, \"source\": src_label}); seen_keys.add(key)\n        if any(k in low for k in (\"how supplied\", \"how-supplied\", \"storage\", \"packaging\", \"handling\")):\n            key = (\"supply_sent\", low[:320])\n            if key not in seen_keys:\n                out[\"how_supplied\"].append({\"paragraph\": para, \"page\": page, \"match_text\": s_text, \"source\": src_label}); seen_keys.add(key)\n        if any(k in low for k in (\"patient counseling\", \"patient information\", \"counseling information\")):\n            key = (\"pc_sent\", low[:320])\n            if key not in seen_keys:\n                out[\"patient_counseling\"].append({\"paragraph\": para, \"page\": page, \"match_text\": s_text, \"source\": src_label}); seen_keys.add(key)\n\n    return out\n\n# ------------------- collect unmatched paragraphs -> OTHER -------------------\ndef _collect_unmatched_paragraphs(pages: List[Dict[str,Any]], result: Dict[str,Any], min_para_chars: int = 40, max_other_chars: Optional[int] = 5000) -> Dict[str,Any]:\n    seen = set()\n    for k, v in result.items():\n        if k.startswith(\"_\") or k in (\"file_id\", \"medicine\", \"creator\", \"version_date\", \"revision_date\", \"special_populations\"):\n            continue\n        if isinstance(v, list):\n            for it in v:\n                cand = (it.get(\"paragraph\") or it.get(\"match_text\") or \"\") or \"\"\n                cand_norm = re.sub(r\"\\s+\", \" \", cand).strip().lower()\n                if cand_norm:\n                    seen.add(cand_norm[:1000])\n        elif isinstance(v, dict):\n            for subk, subv in v.items():\n                if isinstance(subv, dict):\n                    cand = (subv.get(\"paragraph\") or subv.get(\"match_text\") or \"\") or \"\"\n                    cand_norm = re.sub(r\"\\s+\", \" \", cand).strip().lower()\n                    if cand_norm:\n                        seen.add(cand_norm[:1000])\n\n    other_paragraphs = []\n    seen_local = set()\n    for p in pages:\n        page_num = p.get(\"page\")\n        text = p.get(\"text\",\"\") or \"\"\n        paras = [pp.strip() for pp in re.split(r\"\\n\\s*\\n+\", text) if pp and pp.strip()]\n        if not paras:\n            paras = [ln.strip() for ln in text.splitlines() if ln.strip()]\n        for para in paras:\n            para_clean = re.sub(r\"\\s+\", \" \", para).strip()\n            if len(para_clean) < min_para_chars:\n                continue\n            key = para_clean.lower()[:1000]\n            if not key:\n                continue\n            if key in seen:\n                continue\n            if key in seen_local:\n                continue\n            contained = False\n            for s in seen:\n                if len(s) > 40 and s in key:\n                    contained = True; break\n                if len(key) > 40 and key in s:\n                    contained = True; break\n            if contained:\n                continue\n            other_paragraphs.append({\"paragraph\": para_clean, \"page\": page_num, \"source\": \"other\"})\n            seen_local.add(key)\n\n    if not other_paragraphs:\n        full_text = \" \".join([p.get(\"text\",\"\") or \"\" for p in pages])\n        sents = re.split(r'(?<=[\\.\\!\\?])\\s+', full_text)\n        for s in sents:\n            s_clean = re.sub(r\"\\s+\", \" \", s).strip()\n            if len(s_clean) < min_para_chars:\n                continue\n            k = s_clean.lower()[:1000]\n            if k in seen or k in seen_local:\n                continue\n            page = _find_page_for_match(s_clean, pages)\n            other_paragraphs.append({\"paragraph\": s_clean, \"page\": page, \"source\": \"other\"})\n            seen_local.add(k)\n\n    blob = \"\\n\\n\".join([p[\"paragraph\"] for p in other_paragraphs])\n    if max_other_chars is not None and isinstance(max_other_chars, int) and max_other_chars > 0:\n        if len(blob) > max_other_chars:\n            blob = blob[:max_other_chars] + \"\\n\\n...[truncated]\"\n    return {\"blob\": blob, \"paragraphs\": other_paragraphs}\n\n# ------------------- ranking & filter helper -------------------\ndef _section_matches_feature(section_name: str, feature: str) -> bool:\n    if not section_name:\n        return False\n    s = section_name.lower()\n    f = feature.lower()\n    if f in (\"dosage\",):\n        return any(k in s for k in (\"dosage\", \"administration\"))\n    if f in (\"indications\",):\n        return any(k in s for k in (\"indication\", \"indications\", \"usage\", \"use\"))\n    if f in (\"warnings\",):\n        return any(k in s for k in (\"warning\", \"precaution\"))\n    if f in (\"side_effects\",):\n        return any(k in s for k in (\"adverse\", \"side effect\", \"side-effect\"))\n    if f in (\"contraindications\",):\n        return any(k in s for k in (\"contra\", \"who should not\", \"not recommended\"))\n    if f in (\"drug_interactions\",):\n        return any(k in s for k in (\"interaction\", \"drug interaction\", \"concomitant\"))\n    if f in (\"overdosage\",):\n        return any(k in s for k in (\"overdosage\", \"overdose\"))\n    if f in (\"clinical_pharmacology\",):\n        return any(k in s for k in (\"pharmacology\",\"pharmacokinetics\",\"mechanism\"))\n    if f in (\"nonclinical_toxicology\",):\n        return any(k in s for k in (\"nonclinical\",\"toxicology\",\"carcinogenicity\"))\n    if f in (\"clinical_studies\",):\n        return any(k in s for k in (\"study\",\"clinical study\",\"trial\"))\n    if f in (\"references\",):\n        return \"reference\" in s or \"bibliography\" in s\n    if f in (\"how_supplied\",):\n        return any(k in s for k in (\"how supplied\", \"storage\", \"handling\"))\n    if f in (\"patient_counseling\",):\n        return any(k in s for k in (\"patient counseling\", \"patient information\"))\n    if f in (\"description\",):\n        return any(k in s for k in (\"description\", \"composition\", \"formulation\"))\n    return False\n\ndef _filter_and_rank(items: List[Dict[str,Any]], feature: str, sections: List[Dict[str,Any]], pages: List[Dict[str,Any]], top_k: int = 5) -> List[Dict[str,Any]]:\n    if not items:\n        return []\n\n    source_weights = {\n        \"scispacy_ent\": 9.0,\n        \"scispacy\": 5.0,\n        \"ner_ent\": 5.0,\n        \"ner\": 4.0,\n        \"ner_sent\": 2.5,\n        \"regex\": 2.0,\n        \"llm\": 1.0,\n        \"other\": 0.5\n    }\n\n    scored = []\n    section_texts = [(s.get(\"section\",\"\") or \"\", (s.get(\"text\",\"\") or \"\").lower(), s.get(\"start_page\")) for s in (sections or [])]\n\n    for it in items:\n        para = (it.get(\"paragraph\") or \"\") or \"\"\n        mt = (it.get(\"match_text\") or \"\") or \"\"\n        page = it.get(\"page\")\n        src = (it.get(\"source\") or \"\").lower()\n        score = 0.0\n\n        score += source_weights.get(src, 1.0)\n        f = feature.lower()\n        combined = (para + \" \" + mt).lower()\n        if f.replace(\"_\",\" \") in combined:\n            score += 2.0\n        if f == \"side_effects\" and (\"side effect\" in combined or \"adverse\" in combined):\n            score += 1.5\n        if f == \"drug_interactions\" and (\"interaction\" in combined or \"interact\" in combined or \"concomitant\" in combined):\n            score += 1.5\n\n        try:\n            if page:\n                for sec_name, sec_text, sec_start in section_texts:\n                    if sec_start is None:\n                        continue\n                    if abs(sec_start - page) <= 1 and _section_matches_feature(sec_name, feature):\n                        score += 3.0\n                        break\n            for sec_name, sec_text, sec_start in section_texts:\n                if not sec_text:\n                    continue\n                if para and para.strip().lower()[:300] in sec_text:\n                    if _section_matches_feature(sec_name, feature):\n                        score += 3.0\n                        break\n        except Exception:\n            pass\n\n        if feature == \"dosage\":\n            parsed = it.get(\"_dosage_parsed\") or {}\n            if isinstance(parsed, dict) and parsed.get(\"valid\"):\n                score += 2.0\n\n        length = len(para or mt or \"\")\n        if length > 1000:\n            score -= 0.5\n        if length < 80:\n            score += 0.2\n\n        scored.append((score, page if page is not None else 9999, length, it))\n\n    scored.sort(key=lambda x: (-x[0], x[1], x[2]))\n\n    top_items = []\n    seen = set()\n    for sc, pg, ln, it in scored:\n        key = ((it.get(\"paragraph\") or \"\")[:500]).strip().lower()\n        if not key:\n            key = ((it.get(\"match_text\") or \"\")[:500]).strip().lower()\n        if key in seen:\n            continue\n        new_it = dict(it)\n        new_it[\"_rank_score\"] = round(float(sc), 3)\n        top_items.append(new_it)\n        seen.add(key)\n        if len(top_items) >= max(1, int(top_k or 1)):\n            break\n\n    return top_items\n\n# ------------------- Local LLM helpers (optional fallback) -------------------\ndef init_local_llm(model_name: str = \"gpt2\", load_8bit=True):\n    if _LOCAL_LLM_CTX.get(\"ready\") and _LOCAL_LLM_CTX.get(\"name\") == model_name:\n        return True\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM\n        _LOCAL_LLM_CTX[\"tokenizer\"] = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n        model = AutoModelForCausalLM.from_pretrained(model_name)\n        _LOCAL_LLM_CTX.update({\"ready\": True, \"model\": model, \"name\": model_name})\n        return True\n    except Exception as exc:\n        _LOCAL_LLM_CTX.update({\"ready\": False, \"model\": None, \"tokenizer\": None, \"name\": None, \"error\": str(exc)})\n        return False\n\ndef _extract_json_from_text(text: str) -> Tuple[Optional[Dict[str, Any]], Optional[str]]:\n    if not isinstance(text, str):\n        return None, \"model output not text\"\n    text = re.sub(r\"^(?:json)?\\s*\", \"\", text.strip())\n    text = re.sub(r\"\\s*$\", \"\", text.strip())\n    s = text.find(\"{\")\n    e = text.rfind(\"}\")\n    if s != -1 and e != -1 and e > s:\n        candidate = text[s:e+1]\n        try:\n            parsed = json.loads(candidate)\n            return parsed, None\n        except Exception:\n            try:\n                cleaned = re.sub(r\",\\s*}\\s*\", \"}\", candidate)\n                cleaned = re.sub(r\",\\s*\\]\\s*\", \"]\", cleaned)\n                parsed = json.loads(cleaned)\n                return parsed, None\n            except Exception as ee:\n                return None, f\"json parse error: {ee}; raw snippet: {candidate[:300]}\"\n    return None, \"no json-like substring found\"\n\ndef _local_llm_extract_single_feature(snippet_text: str, feature: str, max_new_tokens: int = 256, temperature: float = 0.0):\n    if not _LOCAL_LLM_CTX.get(\"ready\"):\n        ok = init_local_llm()\n        if not ok:\n            return {\"success\": False, \"error\": \"local LLM not available\"}\n    try:\n        tokenizer = _LOCAL_LLM_CTX[\"tokenizer\"]\n        model = _LOCAL_LLM_CTX[\"model\"]\n        import torch\n        prompt = (f\"You are a strict extractor. Extract ONLY the '{feature}' field from the InputText below and RETURN only valid JSON.\\n\"\n                  \"If nothing relevant, return an empty list or null for that field.\\n\"\n                  \"The extracted value should be an array of objects with 'text' and optional 'page'.\\n\"\n                  \"InputText: '''\\n\" + snippet_text.strip()[:30000] + \"\\n'''\\n\"\n                  f\"Return JSON like: {{\\\"{feature}\\\":[{{\\\"text\\\":\\\"...\\\",\\\"page\\\":null}}]}}\")\n        enc = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=min(4096, tokenizer.model_max_length))\n        device = next(model.parameters()).device\n        enc = {k: v.to(device) for k, v in enc.items()}\n        with torch.no_grad():\n            out = model.generate(**enc, max_new_tokens=int(max_new_tokens), do_sample=False)\n        txt = tokenizer.decode(out[0], skip_special_tokens=True)\n        parsed, perr = _extract_json_from_text(txt)\n        if parsed is not None:\n            return {\"success\": True, \"result\": parsed}\n        else:\n            return {\"success\": False, \"error\": \"could not parse JSON\", \"raw\": txt[:2000], \"parse_error\": perr}\n    except Exception as e:\n        return {\"success\": False, \"error\": str(e)}\n\n# ------------------- chunk collector for headings (fallback to sections) -------------------\ndef _collect_chunks_for_heading(heading: str, pages: List[Dict[str,Any]], sections: List[Dict[str,Any]], full_text: str, max_chars: int = 1200) -> List[Dict[str,Any]]:\n    out: List[Dict[str,Any]] = []\n    seen = set()\n    for sec in sections:\n        sec_name = (sec.get(\"section\") or \"\").upper()\n        if not sec_name:\n            continue\n        if heading.upper() in sec_name or any(tok in sec_name for tok in heading.upper().split()):\n            paras = [p.strip() for p in re.split(r\"\\n\\s*\\n+\", sec.get(\"text\",\"\")) if p.strip()]\n            for p in paras:\n                pshort = re.sub(r\"\\s+\", \" \", p)[:max_chars].strip()\n                if not pshort:\n                    continue\n                page = sec.get(\"start_page\") or sec.get(\"end_page\") or _find_page_for_match(pshort, pages)\n                key = pshort.lower()[:500]\n                if key in seen:\n                    continue\n                out.append({\"chunk_text\": pshort, \"page\": page, \"source\": \"collector\"})\n                seen.add(key)\n                if len(out) >= 6:\n                    break\n    if not out:\n        # fallback regex\n        base = heading.split()[0] if heading else \"\"\n        if base:\n            pattern = re.compile(r'(' + re.escape(base) + r'.{0,800})', re.I | re.S)\n            for m in pattern.finditer(full_text):\n                snippet = m.group(0)\n                snippet_clean = re.sub(r\"\\s+\", \" \", snippet).strip()\n                snippet_short = snippet_clean[:max_chars]\n                page = _find_page_for_match(snippet_short, pages)\n                key = snippet_short.lower()[:500]\n                if key in seen:\n                    continue\n                out.append({\"chunk_text\": snippet_short, \"page\": page, })\n                seen.add(key)\n                if len(out) >= 6:\n                    break\n    # deduped return items (no source)\n    final = []\n    seen2 = set()\n    for it in out:\n        k = (it.get(\"chunk_text\") or \"\")[:400].strip().lower()\n        if k and k not in seen2:\n            final.append({\"chunk_text\": it.get(\"chunk_text\"), \"page\": it.get(\"page\")})\n            seen2.add(k)\n    return final\n        \n# ------------------- orchestrator: NER -> regex -> LLM, ranking, build schema -------------------\ndef process_text_json(text_json: Any, use_ner: bool = True, use_local_llm: bool = False, llm_budget_tokens: int = 256) -> Dict[str,Any]:\n    \"\"\"\n    Main entry:\n      - text_json: your OCR JSON (dict/list/string/bytes)\n      - returns schema {\"file_name\":..., \"name\":..., \"sections\": {HEADING: [{\"chunk_text\",\"page\",\"source\"}, ...]}}\n    \"\"\"\n    pages, filename = _pages_from_text_json(text_json)\n    if pages is None:\n        raise ValueError(\"Unrecognized text_json shape. Expecting {'response':[{'filename':..., 'page_info':[{'page':n,'text':...}]}]}\")\n\n    sections = _find_headings_in_pages(pages)\n    # Build full_text aligned with spaCy char offsets\n    full_text, full_page_map = _concat_pages_to_full_text(pages)\n\n    # basic metadata (name = prominent uppercase line OR filename; no brand/generic)\n    front = pages[0].get(\"text\",\"\") if pages else \"\"\n    name = None\n    for ln in (front or \"\").splitlines()[:10]:\n        lnstr = ln.strip()\n        if lnstr and len(lnstr) > 3 and lnstr.isupper():\n            name = lnstr; break\n    if not name:\n        name = os.path.splitext(filename or \"\")[0] if filename else \"UNKNOWN\"\n\n    # Prepare result dict\n    result = {\n        \"file_name\": filename or \"\",\n        \"name\": name or \"\",\n        # per-feature lists that we will populate (items keep 'source' field)\n        \"indications\": [], \"dosage\": [], \"warnings\": [], \"side_effects\": [], \"contraindications\": [],\n        \"drug_interactions\": [], \"overdosage\": [], \"description\": [], \"clinical_pharmacology\": [],\n        \"nonclinical_toxicology\": [], \"clinical_studies\": [], \"references\": [], \"how_supplied\": [], \"patient_counseling\": [],\n        \"special_populations\": {}\n    }\n\n    # 1) NER using scispaCy / spaCy matcher\n    nlp = None; matcher = None; ner_out = {}\n    if use_ner and spacy is not None:\n        try:\n            nlp = _safe_load_spacy(\"en_core_sci_scibert\")\n            if nlp:\n                matcher = _build_feature_matcher(nlp)\n                ner_out = _ner_extract_features(full_text, pages, nlp, matcher)\n        except Exception:\n            traceback.print_exc()\n            ner_out = {}\n\n    # helper to add ner entries to result (deduped) -- preserve source\n    def _add_ner_to_result(key, items):\n        if not items:\n            return\n        seen = { ((it.get(\"paragraph\") or \"\")[:300]).strip() for it in result.get(key, []) }\n        for it in items:\n            para = it.get(\"paragraph\") or it.get(\"match_text\") or ''\n            page = it.get(\"page\")\n            src = it.get(\"source\") or \"ner\"\n            txt = (para or \"\").strip()\n            if txt and ((txt)[:300]).strip() not in seen:\n                entry = {\"paragraph\": txt, \"page\": page, \"source\": src}\n                result.setdefault(key, []).append(entry)\n                seen.add(((txt)[:300]).strip())\n\n    if ner_out:\n        _add_ner_to_result(\"indications\", ner_out.get(\"indications\", []))\n        _add_ner_to_result(\"dosage\", ner_out.get(\"dosage\", []))\n        _add_ner_to_result(\"warnings\", ner_out.get(\"warnings\", []))\n        _add_ner_to_result(\"side_effects\", ner_out.get(\"side_effects\", []))\n        _add_ner_to_result(\"contraindications\", ner_out.get(\"contraindications\", []))\n        _add_ner_to_result(\"drug_interactions\", ner_out.get(\"drug_interactions\", []))\n        _add_ner_to_result(\"overdosage\", ner_out.get(\"overdosage\", []))\n        _add_ner_to_result(\"description\", ner_out.get(\"description\", []))\n        _add_ner_to_result(\"clinical_pharmacology\", ner_out.get(\"clinical_pharmacology\", []))\n        _add_ner_to_result(\"nonclinical_toxicology\", ner_out.get(\"nonclinical_toxicology\", []))\n        _add_ner_to_result(\"clinical_studies\", ner_out.get(\"clinical_studies\", []))\n        _add_ner_to_result(\"references\", ner_out.get(\"references\", []))\n        _add_ner_to_result(\"how_supplied\", ner_out.get(\"how_supplied\", []))\n        _add_ner_to_result(\"patient_counseling\", ner_out.get(\"patient_counseling\", []))\n\n        for sp in ner_out.get(\"special_populations\", []):\n            pop_key = sp.get(\"population\")\n            if pop_key and pop_key not in result[\"special_populations\"]:\n                result[\"special_populations\"][pop_key] = {\"paragraph\": sp.get(\"paragraph\"), \"page\": sp.get(\"page\"), \"source\": sp.get(\"source\", \"ner\")}\n    else:\n        # nothing from ner\n        pass\n\n    # 2) Regex fallback for missing features (preserve source \"regex\")\n    features_to_check = [\"indications\",\"dosage\",\"warnings\",\"side_effects\",\"contraindications\",\n                         \"drug_interactions\",\"overdosage\",\"description\",\"clinical_pharmacology\",\n                         \"nonclinical_toxicology\",\"clinical_studies\",\"references\",\"how_supplied\",\"patient_counseling\"]\n\n    for feat in features_to_check:\n        current = result.get(feat) or []\n        if not current:\n            try:\n                candidates = _regex_search_feature(feat, full_text, pages, sections)\n                if candidates:\n                    seen = set()\n                    for c in candidates:\n                        keytxt = ((c.get(\"paragraph\") or c.get(\"match_text\") or \"\")[:400]).strip()\n                        if keytxt and keytxt not in seen:\n                            # store paragraph+page+source\n                            result.setdefault(feat, []).append({\"paragraph\": c.get(\"paragraph\"), \"page\": c.get(\"page\"), \"source\": c.get(\"source\", \"regex\")})\n                            seen.add(keytxt)\n            except Exception:\n                traceback.print_exc()\n\n    # 3) local LLM fallback for still-missing features (optional)\n    missing_after_regex = [f for f in features_to_check if not result.get(f)]\n    if missing_after_regex and use_local_llm:\n        try:\n            init_local_llm()\n        except Exception:\n            pass\n        for feat in missing_after_regex:\n            relevant_texts = []\n            for sec in sections:\n                if feat.upper() in sec.get(\"section\",\"\").upper() or any(k in sec.get(\"section\",\"\").upper() for k in (\"DOSAGE\",\"ADMINISTRATION\",\"INDICATION\",\"WARN\",\"ADVERSE\",\"CONTRA\",\"INTERACT\",\"OVERDOSE\",\"PHARMACOLOGY\",\"TOXICOLOGY\",\"STUDIES\",\"REFERENCE\",\"HOW SUPPLIED\",\"PATIENT\")):\n                    relevant_texts.append(sec.get(\"text\",\"\"))\n            regex_cands = _regex_search_feature(feat, full_text, pages, sections)\n            for rc in regex_cands[:3]:\n                if rc.get(\"match_text\"):\n                    relevant_texts.append(rc[\"match_text\"])\n            snippet = \"\\n\\n\".join([t for t in relevant_texts if t])[:30000]\n            if not snippet:\n                snippet = full_text[:30000]\n            try:\n                llm_resp = _local_llm_extract_single_feature(snippet, feat, max_new_tokens=llm_budget_tokens)\n            except Exception as e:\n                llm_resp = {\"success\": False, \"error\": str(e)}\n            if llm_resp.get(\"success\"):\n                parsed = llm_resp.get(\"result\", {})\n                arr = parsed.get(feat) or parsed.get(\"results\", {}).get(feat) if isinstance(parsed, dict) else None\n                if not arr:\n                    arr = []\n                normalized = []\n                for v in arr:\n                    if isinstance(v, str):\n                        normalized.append({\"paragraph\": v, \"page\": None, \"source\": \"llm\"})\n                    elif isinstance(v, dict):\n                        txt = v.get(\"text\") or v.get(\"dosage_text\") or v.get(\"match_text\") or v.get(\"paragraph\")\n                        if not txt:\n                            continue\n                        normalized.append({\"paragraph\": txt, \"page\": v.get(\"page\"), \"source\": \"llm\"})\n                if normalized:\n                    seen = { ((it.get(\"paragraph\") or \"\")[:300]).strip() for it in result.get(feat, []) }\n                    for it in normalized:\n                        k = ((it.get(\"paragraph\") or \"\")[:300]).strip()\n                        if k and k not in seen:\n                            result.setdefault(feat, []).append(it)\n                            seen.add(k)\n\n    # post-process dedupe and dosage parsing (preserve source)\n    for k in [\"indications\",\"dosage\",\"warnings\",\"side_effects\",\"contraindications\",\n              \"drug_interactions\",\"overdosage\",\"description\",\"clinical_pharmacology\",\n              \"nonclinical_toxicology\",\"clinical_studies\",\"references\",\"how_supplied\",\"patient_counseling\"]:\n        seen = set(); uniq = []\n        for it in result.get(k, []):\n            key = ((it.get(\"paragraph\") or \"\")[:800]).strip()\n            if key and key not in seen:\n                if k == \"dosage\":\n                    parsed = _parse_and_validate_dosage_text(it.get(\"paragraph\") or \"\")\n                    it[\"_dosage_parsed\"] = parsed\n                uniq.append(it); seen.add(key)\n        result[k] = uniq\n\n    # ------------------- apply TOP_K_BY_FEATURE ranking/filtering -------------------\n    try:\n        for k in TOP_K_BY_FEATURE:\n            topk = TOP_K_BY_FEATURE.get(k, 1)\n            filtered = _filter_and_rank(result.get(k, []), k, sections, pages, top_k=topk)\n            # ensure dosage parsed\n            if k == \"dosage\":\n                for it in filtered:\n                    if \"_dosage_parsed\" not in it:\n                        it[\"_dosage_parsed\"] = _parse_and_validate_dosage_text(it.get(\"paragraph\") or \"\")\n            # keep 'source' and keep paragraph+page only plus _meta\n            cleaned = []\n            for it in filtered:\n                cleaned.append({\n                    \"paragraph\": it.get(\"paragraph\"),\n                    \"page\": it.get(\"page\"),\n                    \"source\": it.get(\"source\", \"extracted\"),\n                    \"_meta\": {k: it.get(\"_dosage_parsed\") if k==\"dosage\" else None, \"_score\": it.get(\"_rank_score\")}\n                })\n            result[k] = cleaned\n    except Exception:\n        traceback.print_exc()\n\n    # Build OTHER bucket\n    try:\n        other = _collect_unmatched_paragraphs(pages, result, min_para_chars=30, max_other_chars=8000)\n        result[\"other\"] = other\n    except Exception:\n        traceback.print_exc()\n        result[\"other\"] = {\"blob\": \"\", \"paragraphs\": []}\n    FEATURE_TO_HEADING = {\n        \"indications\": \"INDICATIONS AND USAGE\",\n        \"dosage\": \"DOSAGE AND ADMINISTRATION\",\n        \"contraindications\": \"CONTRAINDICATIONS\",\n        \"warnings\": \"WARNINGS AND PRECAUTIONS\",\n        \"side_effects\": \"ADVERSE REACTIONS\",\n        \"drug_interactions\": \"DRUG INTERACTIONS\",\n        \"overdosage\": \"OVERDOSAGE\",\n        \"description\": \"DESCRIPTION\",\n        \"clinical_pharmacology\": \"CLINICAL PHARMACOLOGY\",\n        \"nonclinical_toxicology\": \"NONCLINICAL TOXICOLOGY\",\n        \"clinical_studies\": \"CLINICAL STUDIES\",\n        \"references\": \"REFERENCES\",\n        \"how_supplied\": \"HOW SUPPLIED/STORAGE AND HANDLING\",\n        \"patient_counseling\": \"PATIENT COUNSELING INFORMATION\",\n    }\n\n    sections_dict: Dict[str, List[Dict[str,Any]]] = {}\n\n    # Prefer features (NER/regex/llm results) to populate headings\n    for feat, heading in FEATURE_TO_HEADING.items():\n        items = result.get(feat, []) or []\n        if items:\n            chunks = []\n            for it in items:\n                para = (it.get(\"paragraph\") or \"\") or \"\"\n                page = it.get(\"page\")\n                src = it.get(\"source\", \"extracted\")\n                # normalize paragraph whitespace and shorten to a chunk size (optional)\n                chunk = re.sub(r\"\\s+\", \" \", para).strip()\n                if not chunk:\n                    continue\n                chunks.append({\"chunk_text\": chunk[:2000], \"page\": page or 0})\n            if chunks:\n                sections_dict[heading] = chunks\n\n    # For any canonical heading missing, attempt to extract using detected page sections or chunk collector\n    for heading in _ADDITIONAL_HEADINGS:\n        if heading in sections_dict:\n            continue\n        # attempt to collect chunks from detected page sections\n        collected = _collect_chunks_for_heading(heading, pages, sections, full_text, max_chars=1200)\n        if collected:\n            sections_dict[heading] = [{\"chunk_text\": c.get(\"chunk_text\"), \"page\": c.get(\"page\")} for c in collected]\n\n\n    # Add any other detected sections (not canonical)\n    for sec in sections:\n        sec_name = (sec.get(\"section\") or \"\").upper()\n        if not sec_name or sec_name in sections_dict:\n            continue\n        paras = [p.strip() for p in re.split(r\"\\n\\s*\\n+\", sec.get(\"text\",\"\")) if p.strip()]\n        paras_short = []\n        for p in paras[:6]:\n            pshort = re.sub(r\"\\s+\", \" \", p)[:1200].strip()\n            if pshort:\n                page = sec.get(\"start_page\") or _find_page_for_match(pshort, pages)\n                paras_short.append({\"chunk_text\": pshort, \"page\": page})\n        if paras_short:\n            sections_dict[sec_name] = paras_short\n\n    # Finally, ensure OTHER is present and contains unmatched paragraphs (preserve source)\n    other_pars = result.get(\"other\", {}).get(\"paragraphs\", [])\n    if other_pars:\n        sections_dict.setdefault(\"OTHER\", [])\n        for p in other_pars:\n            sections_dict.setdefault(\"OTHER\", [])\n            sections_dict[\"OTHER\"].append({\"chunk_text\": p.get(\"paragraph\"), \"page\": p.get(\"page\")})\n\n\n    # Final object\n    out = {\"file_name\": filename or \"\", \"name\": name or \"\", \"sections\": sections_dict}\n    return out\n\ndef process_text_json_bulk(text_json: Any,\n                           use_ner: bool = True,\n                           use_local_llm: bool = False,\n                           llm_budget_tokens: int = 256,\n                           process_all: bool = False) -> Dict[str, Any]:\n    \"\"\"\n    Processes either the first file (default) or all files in the input `text_json`.\n    Returns: {\"data\": [<output for file1>, <output for file2>, ...]}\n    \"\"\"\n    # normalize input -> parsed dict/list (reuse logic from _pages_from_text_json)\n    parsed = None\n    if isinstance(text_json, (bytes, bytearray)):\n        try:\n            parsed = json.loads(text_json.decode(\"utf-8\", errors=\"ignore\"))\n        except Exception:\n            parsed = json.loads(text_json)\n    elif isinstance(text_json, str):\n        try:\n            parsed = json.loads(text_json)\n        except Exception:\n            parsed = None\n    elif isinstance(text_json, dict) or isinstance(text_json, list):\n        parsed = text_json\n    if parsed is None:\n        return {\"data\": []}\n\n    # extract response list\n    resp = None\n    if isinstance(parsed, dict) and \"response\" in parsed and isinstance(parsed[\"response\"], list):\n        resp = parsed[\"response\"]\n    elif isinstance(parsed, list):\n        resp = parsed\n    else:\n        return {\"data\": []}\n\n    outputs = []\n    # if only want first file (default) and at least one file present\n    if not process_all:\n        entry = resp[0] if resp else None\n        if entry is None:\n            return {\"data\": []}\n        try:\n            out = process_text_json({\"response\": [entry]}, use_ner=use_ner, use_local_llm=use_local_llm, llm_budget_tokens=llm_budget_tokens)\n            return {\"data\": [out]}\n        except Exception as e:\n            return {\"data\": [{\"file_name\": entry.get(\"filename\") or entry.get(\"name\") or \"\", \"error\": str(e)}]}\n\n    # process all entries\n    for entry in resp:\n        try:\n            out = process_text_json({\"response\": [entry]}, use_ner=use_ner, use_local_llm=use_local_llm, llm_budget_tokens=llm_budget_tokens)\n            outputs.append(out)\n        except Exception as e:\n            outputs.append({\"file_name\": entry.get(\"filename\") or entry.get(\"name\") or \"\", \"error\": str(e)})\n    return {\"data\": outputs}\n\n\ndef _pages_to_input_json(pages: List[Dict[str,Any]], filename_hint: Optional[str]=None):\n    page_info = [{\"page\": p.get(\"page\"), \"text\": p.get(\"text\")} for p in pages]\n    return {\"response\": [{\"filename\": filename_hint or \"\", \"page_info\": page_info}]}\n    \ndef extract_with_ner_then_regex_then_llm(sections, pages, filename_hint=None,\n                                         use_ner=True, use_hf=False, use_local_llm=False,\n                                         llm_budget_tokens: int = 256, process_all: bool = False):\n    \"\"\"\n    Compatibility wrapper updated to optionally process multiple file entries.\n    - If `pages` is provided (old behaviour) it will process that single pages-list.\n    - Otherwise if `sections` is an OCR/json-like object (with 'response'), this will call process_text_json_bulk.\n    - `process_all=True` will process all files in the response list.\n    \"\"\"\n    # old behaviour: caller passed pages/list -> keep working\n    if pages:\n        input_json = _pages_to_input_json(pages, filename_hint)\n        out = process_text_json(input_json, use_ner=use_ner, use_local_llm=use_local_llm, llm_budget_tokens=llm_budget_tokens)\n        # map to old shape (single-file)\n        heading_to_feature = {\n            \"INDICATIONS AND USAGE\": \"indications\",\n            \"DOSAGE AND ADMINISTRATION\": \"dosage\",\n            \"WARNINGS AND PRECAUTIONS\": \"warnings\",\n            \"ADVERSE REACTIONS\": \"side_effects\",\n            \"CONTRAINDICATIONS\": \"contraindications\",\n            \"DRUG INTERACTIONS\": \"drug_interactions\",\n            \"OVERDOSAGE\": \"overdosage\",\n            \"DESCRIPTION\": \"description\",\n            \"CLINICAL PHARMACOLOGY\": \"clinical_pharmacology\",\n            \"NONCLINICAL TOXICOLOGY\": \"nonclinical_toxicology\",\n            \"CLINICAL STUDIES\": \"clinical_studies\",\n            \"REFERENCES\": \"references\",\n            \"HOW SUPPLIED/STORAGE AND HANDLING\": \"how_supplied\",\n            \"PATIENT COUNSELING INFORMATION\": \"patient_counseling\",\n        }\n        feature_to_heading = {v: k for k, v in heading_to_feature.items()}\n        result = {\"file_id\": out.get(\"file_name\") or filename_hint or \"\"}\n        result[\"medicine\"] = {\"brand\": out.get(\"name\") or \"\", \"generic\": \"\"}\n        for feature in feature_to_heading:\n            heading = feature_to_heading[feature]\n            items = out.get(\"sections\", {}).get(heading, [])\n            mapped = [{\"paragraph\": it.get(\"chunk_text\") or \"\", \"page\": it.get(\"page\")} for it in items]\n            result[feature] = mapped\n        other = out.get(\"sections\", {}).get(\"OTHER\", [])\n        result[\"other\"] = {\"paragraphs\": [{\"paragraph\": it.get(\"chunk_text\"), \"page\": it.get(\"page\")} for it in other],\n                           \"blob\": out.get(\"sections\", {}).get(\"OTHER\", [])}\n        result[\"_diagnostics\"] = {\"compat_wrapper\": True}\n        return result\n\n    # new behaviour: caller passed full OCR/json object in `sections` param (legacy name)\n    return process_text_json_bulk(sections or {}, use_ner=use_ner, use_local_llm=use_local_llm,\n                                  llm_budget_tokens=llm_budget_tokens, process_all=process_all)\n\n\n# ------------------- small convenience wrapper if user wants to call process_text_json with JSON string/file -------------------\ndef process_text_json_from_file(path: str, use_ner: bool = True, use_local_llm: bool = False):\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        j = json.load(f)\n    return process_text_json(j, use_ner=use_ner, use_local_llm=use_local_llm)\n\n\nexample_ocr = {'response': [\n    {'filename': 'AspirinImg.pdf', 'page_info': [{'page': 1, 'text': 'VALUMEDS ASPIRIN- aspirin tablet, delayed release\\nSPIRIT PHARMACEUTICALS LLC\\n\\nASPIRIN 325 MG TABLETS\\n\\nActive ingredient (in each tablet)\\nAspirin (NSAID)* 325 mg\\n*nonsteroidal anti-inflammatory drug\\n\\nPurpose\\nPain reliever\\n\\n¢ temporary relieves minor aches and pains due to:\\ne headache\\ne minor arthritis pain\\n¢ toothache\\ne menstrual pain\\n\\n© colds\\n\\n* or as recommended by a doctor\\n\\nReye’s syndrome: Children and teenagers who have or are recovering from chicken\\npox or flu-like symptoms should not use this product. When using this product, if\\nchanges in behavior with nausea and vomiting occur, consult a doctor because these\\nsymptoms could be an early sign of Reye’s syndrome, a rare but serious illness.\\n\\nAllergy alert: Aspirin may cause a severe allergic reaction which may include:\\ne hives\\n\\n¢ shock\\n\\ne facial swelling\\n\\ne asthma (wheezing)\\n\\nStomach bleeding warning: This product contains an NSAID, which may cause\\nstomach bleeding.\\n\\nThe chance is higher if you\\n\\ne are age 60 or older\\n\\ne have had stomach ulcers or bleeding problems\\n\\ntake a blood thinning (anticoagulant) or steroid drug\\n\\ntake other drugs containing prescription or nonprescription NSAIDS(aspirin,\\nibuprofen, naproxen or others)\\n\\ne have 3 or more alcoholic drinks every day while using this product\\n\\ne take more or for a longer time than directed.\\n\\nDo not use\\n\\nif you have ever had an allergic reaction to aspirin or any other pain reliever/fever'}, {'page': 2, 'text': 'reducer\\n\\nAsk a doctor before use if\\nstomach bleeding warning applies to you\\nyou have a history of stomach problems, such as heartburn\\n\\nyou have high blood pressure, heart disease, liver cirrhosis, or kidney disease\\nyou are taking a diuretic\\nyou have asthma\\n\\nAsk a doctor or phamracist before use if you are\\ntaking a prescription drug for diabetis, gout or arthritis\\n\\nStop use and ask a doctor if\\n\\nallergic reaction occurs, Seek medical help right away.\\nyou experience any of the following signs of stomach bleeding:\\nfeel faint\\n\\nvomit blood\\n\\nhave bloody or black stools\\n\\nhave stomach pain that does not get better\\n\\npain gets worse or lasts more than 10 days\\n\\nredness or swelling is present\\n\\nany new symptom appear\\n\\nringing in the ears or a loss of hearing occurs\\n\\nThese could be signs of a serious condition.\\n\\nIf pregnant or breast-feeding,\\n\\nask a health professional before use. It is especially important not to use aspirin during\\nthe last 3 months of pregnancy unless definitely directed to do so by a doctor because it\\nmay cause problems in the unborn child or complications during delivery.\\n\\nKeep out of reach of children\\n\\nIn case of overdose, get medical help or contact a Poison Control! Center right away.\\n1(800)222-1222\\n\\nDirections\\n\\ne adults and children 12 years of age and over: take 1 to 2 tablets every 4 hours, while\\nsymptoms persist. Drink a full glass of water with each dose.\\n\\ne do not take more than 12 tablets in 24 hours unless directed by a doctor\\n\\ne children under 12 years of age: ask a doctor'}, {'page': 3, 'text': 'Other information\\n\\nstore below 25 °C (77 OF)\\n\\ne¢ Tampet Evident Feature: Do not use ff printed inner-seal beneath cap is missing or\\nbroken\\n\\ncorn starch, croscarmellose sodium,D&C yellow #10 aluminum lake, FD&C yellow #6\\naluminum lake, hypromellose, methacrylic acid copolymer, microcrystalline\\ncellulose,mineral oil, polysorbate 80, simethicone, sodium hydroxide, sodium lauryl\\nsulfate, talc,titanium dioxide, triethyl citrate\\n\\nQuestions or comments?\\n1(888)333-9792\\n\\nPRINCIPAL DISPLAY PANEL\\nVALUMEDS ™\\nCompare to the active ingredient in ECOTRIN ® TABLETS*\\n\\nREGULAR STRENGTH\\nASPIRIN\\n\\nPAIN RELIEVER (NSAID)\\nSAFETY COATED\\n\\nPROTECT AGAINST\\nSTOMACH UPSET\\n100 TABLETS'}, {'page': 4, 'text': 'vi\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n+ protects against stomach upset\\n\\nsafety coated\\n\\nnem qe aft 8 ou ew\\nroad si BYDSHO BLL\\nfCusn oye Aep Lave sy atounge eva 7) Seem ured\\n\\n        \\n\\n \\n\\n‘wow\\nJa OROUBE\\n\\n   \\n\\nops xRuep\\n‘pox Oes 9u} Jo suN0 DU) Yue ry eC INAS\\nAq pqnUuBp 0 punpenuew yi S| prov Siu\\n\\nTvl\\n\\n    \\n\\n(NSAID)\\n\\nsO] e Aq peRowuKes $20\\nopou _ uodio seu\\na se\\n‘oj sud put sag o BvOp AE\\n$05)\\n\\nASPIR\\nis\\n\\ncoated .\\n* protects against stomach upset\\n\\n100 tablets (szsmgeacry\\n\\nsafety\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nItem Code (Source) NDC:682 10-2500\\n\\nHUMAN OTC DRUG\\n\\nORAL\\n\\n \\n\\nVALU MEDS ASPIRIN\\naspirin tablet, delayed release\\nProduct Information\\n\\nProduct Type\\nRoute of Administration'}, {'page': 5, 'text': 'Active Ingredient/Active Moiety\\n\\nIngredient Name Basis of Strength\\nASPIRIN (UNII: RIGCO5Y76E) (ASPIRIN - UNII:RL6CO5Y76E) ASPIRIN\\n\\nInactive Ingredients\\nIngredient Name\\nMICROCRYSTALLINE CELLULOSE (UNI; OP1R32D61U)\\nMINERAL OIL (UNI: T5L8T28FGP)\\nPOLYSORBATE 80 (UNII: 6(0ZP39ZG8H)\\nDIMETHIC ONE (UNII: 92RU3N3Y10)\\nSODIUM HYDROXIDE (UNII: 55X04QC32!)\\nSODIUM LAURYL SULFATE (UNII: 368GB5141))\\nTALC (UNII: 7SEV7J4R1U)\\nTITANIUM DIOXIDE (UNII: 15FIX9V2)P)\\nTRIETHYL CITRATE (UNII: 8296QXD6UM)\\nMETHACRYLIC ACID - ETHYL ACRYLATE COPOLYMER (1:1) TYPE A (UNII: NX76LV5T8})\\nSTARCH, CORN (UNII: 08232NY3S))\\nCROSCARMELLOSE SODIUM (UNI: MZ28OL1HH48)\\nD&C YELLOW NO. 10 (UNII: 35SW5USQ3G)\\nFD&C YELLOW NO. 6 (UNII: H77VEI93A8)\\nHYPROMELLOSES (UNII: 3NXW29V3WO)\\n\\nProduct Characteristics\\n\\nStrength\\n325 mg\\n\\nStrength\\n\\nColor orange Score no score\\nShape ROUND. Size 10mm\\nFlavor Imprint Code 7\\nContains\\nPackaging\\nAes Marketing Start Marketing End\\n# item Code Package Description Date Date\\nNDC:68210- i\\n1 2500-1 1 in 1 CARTON 03/10/2020\\n1 100 in 1 BOTTLE; Type 0: Not a Combination\\nProduct\\n\\nMarketing Information\\n\\nMarketing Application Number or Monograph Marketing Start Marketing End\\n\\nCategory Citation Date Date\\nOTC Monograph Drug M013 03/10/2020\\n\\nLabeler = SPIRIT PHARMACEUTICALS LLC (179621011)'}, {'page': 6, 'text': 'Revised: 12/2024 SPIRIT PHARMACEUTICALS LLC'}]}, \n    {'filename': 'AspirinTxt.pdf', 'page_info': [{'page': 1, 'text': 'VALUMEDS ASPIRIN- aspirin tablet, delayed release  \\nSPIRIT PHARMACEUTICALS LLC\\n\\n----------\\n\\nASPIRIN 325 MG TABLETS\\n\\nActive ingredient (in each tablet)\\n\\nAspirin (NSAID)* 325 mg\\n\\n*nonsteroidal anti-inflammatory drug\\n\\nPurpose\\n\\nPain reliever\\n\\ntemporary relieves minor aches and pains due to: \\nheadache \\nminor arthritis pain\\ntoothache\\nmenstrual pain\\ncolds\\n or as recommended by a doctor\\n\\nReye’s syndrome:  Children and teenagers who have or are recovering from chicken\\npox or flu-like symptoms should not use this product.  When using this product, if\\nchanges in behavior with nausea and vomiting occur, consult a doctor because these\\nsymptoms could be an early sign of Reye’s syndrome, a rare but serious illness. \\n\\nAllergy alert:   Aspirin may cause a severe allergic reaction which may include:\\n\\nhives\\nshock\\nfacial swelling\\nasthma (wheezing)\\n\\nStomach bleeding warning:   This product contains an NSAID, which may cause \\nstomach bleeding.\\n\\n  The chance is higher if you\\n\\nare age 60 or older\\n have had stomach ulcers or bleeding problems\\n take a blood thinning (anticoagulant) or steroid drug\\n take other drugs containing prescription or nonprescription NSAIDS(aspirin,\\nibuprofen, naproxen or others)\\n have 3 or more alcoholic drinks every day while using this product\\ntake more or for a longer time than directed.\\n\\nDo not use\\n\\nif you have ever had an allergic reaction to aspirin or any other pain reliever/fever'}, {'page': 2, 'text': 'reducer\\n\\nAsk a doctor before use if\\n\\nstomach bleeding warning applies to you\\nyou have a history of stomach problems, such as heartburn\\nyou have high blood pressure, heart disease, liver cirrhosis, or kidney disease\\nyou are taking a diuretic\\nyou have asthma\\n\\nAsk a doctor or phamracist before use if you are\\n\\ntaking a prescription drug for diabetis, gout or arthritis\\n\\nStop use and ask a doctor if\\n\\nallergic reaction occurs, Seek medical help right away.\\nyou experience any of the following signs of stomach bleeding:\\nfeel faint \\nvomit blood\\nhave bloody or black stools \\nhave stomach pain that does not get better\\npain gets worse or lasts more than 10 days\\nredness or swelling is present\\nany new symptom appear\\nringing in the ears or a loss of hearing occurs\\n\\nThese could be signs of a serious condition.\\n\\nIf pregnant or breast-feeding,\\n\\nask a health professional before use. It is especially important not to use aspirin during\\nthe last 3 months of pregnancy unless definitely directed to do so by a doctor because it\\nmay cause problems in the unborn child or complications during delivery.\\n\\nKeep out of reach of children\\n\\nIn case of overdose, get medical help or contact a Poison Control Center right away.\\n1(800)222-1222\\n\\nDirections\\n\\nadults and children 12 years of age and over: take 1 to 2 tablets every 4 hours, while\\nsymptoms persist. Drink a full glass of water with each dose.\\n do not take more than 12 tablets in 24 hours unless directed by a doctor\\n children under 12 years of age: ask a doctor'}, {'page': 3, 'text': 'Other information\\n\\n0\\n0\\n\\nstore below 25 C (77 F)\\nTampet Evident Feature: Do not use if printed inner-seal beneath cap is missing or\\nbroken\\n\\ncorn starch, croscarmellose sodium,D&C yellow #10 aluminum lake, FD&C yellow #6\\naluminum lake, hypromellose, methacrylic acid copolymer, microcrystalline\\ncellulose,mineral oil, polysorbate 80, simethicone, sodium hydroxide, sodium lauryl\\nsulfate, talc,titanium dioxide, triethyl citrate\\n\\nQuestions or comments?\\n\\n1(888)333-9792\\n\\nPRINCIPAL DISPLAY PANEL\\n\\nTM\\n\\nVALUMEDS \\n\\nCompare to the active ingredient in ECOTRIN ® TABLETS*\\n\\nREGULAR STRENGTH\\n\\nASPIRIN\\n\\nPAIN RELIEVER (NSAID)\\n\\nSAFETY COATED \\n\\nPROTECT AGAINST\\n\\nSTOMACH UPSET\\n\\n100 TABLETS'}, {'page': 4, 'text': 'VALUMEDS ASPIRIN  \\n\\naspirin tablet, delayed release\\n\\nProduct Information\\n\\nProduct Type\\nHUMAN OTC DRUG\\nItem Code (Source)\\nNDC:68210-2500\\n\\nRoute of Administration\\nORAL'}, {'page': 5, 'text': 'Active Ingredient/Active Moiety\\n\\nIngredient Name\\nBasis of Strength\\nStrength\\n\\nASPIRIN  (UNII: R16CO5Y76E) (ASPIRIN - UNII:R16CO5Y76E)\\nASPIRIN\\n325 mg\\n\\nInactive Ingredients\\n\\nIngredient Name\\nStrength\\n\\nMICROCRYSTALLINE CELLULOSE  (UNII: OP1R32D61U)\\n \\n\\nMINERAL OIL  (UNII: T5L8T28FGP)\\n \\n\\nPOLYSORBATE 80  (UNII: 6OZP39ZG8H)\\n \\n\\nDIMETHICONE  (UNII: 92RU3N3Y1O)\\n \\n\\nSODIUM HYDROXIDE  (UNII: 55X04QC32I)\\n \\n\\nSODIUM LAURYL SULFATE  (UNII: 368GB5141J)\\n \\n\\nTALC  (UNII: 7SEV7J4R1U)\\n \\n\\nTITANIUM DIOXIDE  (UNII: 15FIX9V2JP)\\n \\n\\nTRIETHYL CITRATE  (UNII: 8Z96QXD6UM)\\n \\n\\nMETHACRYLIC ACID - ETHYL ACRYLATE COPOLYMER (1:1) TYPE A  (UNII: NX76LV5T8J)\\n \\n\\nSTARCH, CORN  (UNII: O8232NY3SJ)\\n \\n\\nCROSCARMELLOSE SODIUM  (UNII: M28OL1HH48)\\n \\n\\nD&C YELLOW NO. 10  (UNII: 35SW5USQ3G)\\n \\n\\nFD&C YELLOW NO. 6  (UNII: H77VEI93A8)\\n \\n\\nHYPROMELLOSES  (UNII: 3NXW29V3WO)\\n \\n\\nProduct Characteristics\\n\\nColor\\norange\\nScore\\nno score\\n\\nShape\\nROUND\\nSize\\n10mm\\n\\nFlavor\\nImprint Code\\nT\\n\\nContains\\n    \\n\\nPackaging\\n\\n#\\nItem Code\\nPackage Description\\nMarketing Start\\n\\nMarketing End\\n\\nDate\\n\\nDate\\n\\n1   NDC:68210-\\n\\n2500-1\\n1 in 1 CARTON\\n03/10/2020\\n\\n1\\n100 in 1 BOTTLE; Type 0: Not a Combination\\nProduct\\n\\nMarketing Information\\n\\nMarketing\\n\\nApplication Number or Monograph\\n\\nMarketing Start\\n\\nMarketing End\\n\\nCategory\\n\\nCitation\\n\\nDate\\n\\nDate\\n\\nOTC Monograph Drug\\nM013\\n03/10/2020\\n\\nLabeler -  SPIRIT PHARMACEUTICALS LLC (179621011)'}, {'page': 6, 'text': 'SPIRIT PHARMACEUTICALS LLC\\n \\nRevised: 12/2024'}]}]}\n\n# import json\n\n\n# out = process_text_json(example_ocr, use_ner=True, use_local_llm=False)\n\n# out = process_text_json_bulk(example_ocr, process_all=True)   # returns {\"data\": [out1, out2, ...]}\n\n# print(json.dumps(out, indent=2, ensure_ascii=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T20:18:41.722068Z","iopub.execute_input":"2025-08-26T20:18:41.722295Z","iopub.status.idle":"2025-08-26T20:18:42.949956Z","shell.execute_reply.started":"2025-08-26T20:18:41.722278Z","shell.execute_reply":"2025-08-26T20:18:42.949310Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# DATA = {\n#     \"data\":\n#     [\n#         {\n#       \"file_name\": \"AspirinImg.pdf\",\n#       \"name\": \"VALUMEDS ASPIRIN 325 MG\",\n#       \"sections\": {\n#         \"INDICATIONS AND USAGE\": [\n#           {\n#             \"chunk_text\": \"Aspirin 325 mg tablets — analgesic for temporary relief of minor aches and pains such as headache, minor arthritis pain, toothache, and menstrual pain; also used for colds or as recommended by a doctor.\",\n#             \"page\": 1\n#           },\n#           {\n#             \"chunk_text\": \"Provides temporary relief of pain and is intended for short-term symptomatic relief when used as directed on the label or by a physician.\",\n#             \"page\": 1\n#           }\n#         ],\n#         \"DOSAGE AND ADMINISTRATION\": [\n#           {\n#             \"chunk_text\": \"Each tablet contains 325 mg of aspirin. Use according to label directions or as directed by a healthcare professional. Do not exceed recommended dose or duration.\",\n#             \"page\": 1\n#           },\n#           {\n#             \"chunk_text\": \"If symptoms persist or worsen, consult a doctor. Adults and adolescents should follow package dosing; consult physician for pediatric dosing.\",\n#             \"page\": 2\n#           }\n#         ],\n#         \"CONTRAINDICATIONS\": [\n#           {\n#             \"chunk_text\": \"Do not use if you have had an allergic reaction to aspirin or any other pain reliever/fever reducer (NSAID). History of severe hypersensitivity to aspirin is a contraindication.\",\n#             \"page\": 1\n#           }\n#         ],\n#         \"WARNINGS AND PRECAUTIONS\": [\n#           {\n#             \"chunk_text\": \"Reye’s syndrome warning: Children and teenagers who have or are recovering from chicken pox or flu-like symptoms should not use this product. Seek medical attention if changes in behavior with nausea and vomiting occur.\",\n#             \"page\": 1\n#           },\n#           {\n#             \"chunk_text\": \"Stomach bleeding warning: This product contains an NSAID and may cause stomach bleeding. Risk increases with age (over 60), history of ulcers, anticoagulant or steroid use, concurrent NSAIDs, heavy alcohol use, or taking more than directed.\",\n#             \"page\": 1\n#           }\n#         ],\n#         \"OTHER\": [\n#           {\n#             \"chunk_text\": \"Do not use if you have ever had an allergic reaction to aspirin or any other pain reliever/fever reducer.\",\n#             \"page\": 1\n#           },\n#           {\n#             \"chunk_text\": \"If stomach bleeding warning applies to you or you have history of heartburn or stomach problems, consult a doctor before use.\",\n#             \"page\": 2\n#           }\n#         ]\n#     }\n# },\n#         {\n#       \"file_name\": \"IbuprofenImg.pdf\",\n#       \"name\": \"VALUMEDS IBUPROFEN 200 MG\",\n#       \"sections\": {\n#         \"INDICATIONS AND USAGE\": [\n#           {\n#             \"chunk_text\": \"Ibuprofen 200 mg tablets — used for the relief of minor aches and pains due to headache, muscular aches, backache, menstrual cramps, minor arthritis pain, toothache, and the common cold.\",\n#             \"page\": 1\n#           },\n#           {\n#             \"chunk_text\": \"Also reduces fever and provides temporary relief when used according to the package directions or a doctor’s advice.\",\n#             \"page\": 1\n#           }\n#         ],\n#         \"DOSAGE AND ADMINISTRATION\": [\n#           {\n#             \"chunk_text\": \"Each tablet contains 200 mg of ibuprofen. Adults and children 12 years and over: take 1 tablet every 4 to 6 hours while symptoms persist. If pain or fever does not respond, 2 tablets may be used.\",\n#             \"page\": 1\n#           },\n#           {\n#             \"chunk_text\": \"Do not exceed 6 tablets in 24 hours unless directed by a doctor. Children under 12: consult a physician.\",\n#             \"page\": 2\n#           }\n#         ],\n#         \"CONTRAINDICATIONS\": [\n#           {\n#             \"chunk_text\": \"Do not use if you have had an allergic reaction to ibuprofen, aspirin, or other NSAIDs. Contraindicated in patients with a history of asthma, urticaria, or allergic-type reactions after taking NSAIDs.\",\n#             \"page\": 1\n#           }\n#         ],\n#         \"WARNINGS AND PRECAUTIONS\": [\n#           {\n#             \"chunk_text\": \"Stomach bleeding warning: This product contains an NSAID, which may cause severe stomach bleeding. Risk factors include age over 60, prior ulcers, concurrent anticoagulant use, or taking more than directed.\",\n#             \"page\": 1\n#           },\n#           {\n#             \"chunk_text\": \"Heart attack and stroke warning: NSAIDs, except aspirin, increase the risk of heart attack, heart failure, and stroke. Risk is higher with long use or in people with heart disease.\",\n#             \"page\": 2\n#           }\n#         ],\n#         \"OTHER\": [\n#           {\n#             \"chunk_text\": \"Do not use right before or after heart surgery.\",\n#             \"page\": 1\n#           },\n#           {\n#             \"chunk_text\": \"Ask a doctor before use if you have high blood pressure, heart disease, liver cirrhosis, kidney disease, or are taking diuretics.\",\n#             \"page\": 2\n#           }\n#         ]\n#     }\n# }\n# ]\n# }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T20:18:42.950706Z","iopub.execute_input":"2025-08-26T20:18:42.951206Z","iopub.status.idle":"2025-08-26T20:18:42.957268Z","shell.execute_reply.started":"2025-08-26T20:18:42.951187Z","shell.execute_reply":"2025-08-26T20:18:42.956697Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# required packages:\n# pip install langchain sentence-transformers pinecone-client\n\nimport os\nimport json\nimport hashlib\nfrom datetime import datetime\nfrom typing import List\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom pinecone import Pinecone, ServerlessSpec\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\npinecone_key = user_secrets.get_secret(\"R_PINECONE_KEY\")\n\n# Pinecone Configuration\nPINECONE_API_KEY = pinecone_key\n\nos.environ[\"PINECONE_API_KEY\"] = pinecone_key\n\nif not PINECONE_API_KEY:\n    raise RuntimeError(\"Set PINECONE_API_KEY in environment\")\n\n# Embeddings (HuggingFace wrapper)\nHF_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\nembeddings = HuggingFaceEmbeddings(model_name=HF_MODEL)\n\ndef push_into_db(DATA, user_id):\n    dimension = 384\n    def build_chunks_from_data(data):\n        chunks = []\n        file_name = data.get(\"file_name\", \"unknown.pdf\")\n        doc_name = data.get(\"name\", \"unknown\")\n        sections = data.get(\"sections\", {})\n        for section_name, items in sections.items():\n            for item in items:\n                chunk_text = item.get(\"chunk_text\", \"\").strip()\n                \n                page = item.get(\"page\")\n                if not chunk_text:\n                    continue\n                metadata = {\n                    \"file_name\": file_name.lower(),\n                    \"page\": int(page) if page is not None else 0,\n                    \"name\": doc_name.lower(),\n                    \"topic\": section_name.lower()\n                }\n                record = {\n                    \"name\": doc_name.lower(),\n                    \"topic\": section_name.lower(),\n                    \"chunk_text\": chunk_text.lower(),\n                    \"metadata\": metadata\n                }\n                chunks.append(record)\n        return chunks\n    \n    chunks = []\n    \n    for x in DATA.get(\"data\", []):\n        chunk = build_chunks_from_data(x)\n        chunks+=chunk\n\n    if not chunks:\n        print(\"Empty Dataset. No chunks are created\")\n        return\n        \n    print(\"Prepared chunks:\", len(chunks))\n\n    index_name = \"alphawell\"\n\n    # namespace: User_IDXXXYYYY-MM-DD_HH:MM\n    now = datetime.now()\n    namespace = f\"{user_id}XXX{now.strftime('%Y-%m-%d_%H-%M-%S')}\"\n\n    try:\n        pc = Pinecone(api_key=PINECONE_API_KEY)\n        # pc = Pinecone()\n        print(\"Successful init\")\n    except e:\n        print(\"error initialiing PC\", e)\n    try:\n        existing = pc.list_indexes().names()\n    except Exception:\n        existing = pc.list_indexes()\n\n    if index_name not in existing:\n        try:\n            if PINECONE_REGION:\n                pc.create_index(\n                    name=index_name,\n                    dimension=dimension,\n                    metric=\"cosine\",\n                    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n                )\n            else:\n                pc.create_index(name=index_name, dimension=dimension, metric=\"cosine\")\n            print(\"Index created:\", index_name)\n        except Exception as e:\n            # fallback: retry without spec (project default region)\n            print(\"Create-with-spec failed, retrying without spec:\", str(e))\n            pc.create_index(name=index_name, dimension=dimension, metric=\"cosine\",spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))\n            print(\"Index created (default region):\", index_name)\n    else:\n        print(\"Index exists:\", index_name)\n\n    index = pc.Index(index_name)\n        \n    def make_id_from_text(text: str, prefix: str = None):\n        h = hashlib.sha1(text.encode(\"utf-8\")).hexdigest()[:20]\n        return f\"{prefix + '_' if prefix else ''}{h}\"\n    \n    BATCH = 20\n    total = 0\n    for i in range(0, len(chunks), BATCH):\n        batch = chunks[i:i+BATCH]\n        texts = [f\"medicine: {c['name']} - topic: {c['topic']} - context: {c['chunk_text']}\" for c in batch]\n        embs = embeddings.embed_documents(texts)  # list[list[float]]\n        vectors = []\n        for rec, emb, text in zip(batch, embs, texts):\n            # deterministic id\n            vec_id = make_id_from_text(rec[\"chunk_text\"], prefix=user_id)\n            meta = rec[\"metadata\"].copy()\n            # store original chunk text in metadata too\n            meta[\"chunk_text\"] = text\n            meta[\"name\"] = rec.get(\"name\")\n            meta[\"topic\"] = rec.get(\"topic\")\n            vectors.append((vec_id, emb, meta))\n            total += 1\n        index.upsert(vectors=vectors, namespace=namespace)\n        print(f\"Upserted batch {(i//BATCH)+1}, items={len(vectors)}\")\n    \n    print(f\"Total upserted: {total}\")\n    print(\"Index_name:\", index_name)\n    print(\"Namespace:\", namespace)\n    \n    # returnable result\n    # result = {\"namespace\": namespace}\n\n    return namespace\n\n# print(push_into_db(DATA, \"revanth\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T20:18:42.959854Z","iopub.execute_input":"2025-08-26T20:18:42.960095Z","iopub.status.idle":"2025-08-26T20:18:44.791350Z","shell.execute_reply.started":"2025-08-26T20:18:42.960077Z","shell.execute_reply":"2025-08-26T20:18:44.790684Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from fastapi import FastAPI, UploadFile, File, Form\nfrom pydantic import BaseModel\nimport uvicorn\nfrom pyngrok import ngrok\nfrom fastapi.middleware.cors import CORSMiddleware\nimport nest_asyncio\n# import pinecone\n# from langchain.vectorstores import Pinecone\n\napp = FastAPI()\n\norigins=[\"*\"]\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"]\n)\n\n# Preprocesses the PDF and Creates index in Vector DB\n@app.post(\"/upload\")\nasync def upload_multiple(files: List[UploadFile] = File(...), user_id: str = Form(...)):\n    print(\"User_ID: \", user_id)\n    processed = []\n    for f in files:\n        pdf_bytes = await f.read()\n        result = preprocess_text_pdf(pdf_bytes)\n        pages_info = result.get(\"response\", [])\n        processed.append({\"filename\": f.filename, \"page_info\": pages_info})\n    text_json = {\"response\": processed}\n\n    # text to JSON\n    json_struct = process_text_json_bulk(text_json, process_all=True)\n\n    # Pushing into Pinecone\n    namespace = push_into_db(json_struct, user_id)\n\n    # namespace = \"revanthXXX2025-08-25_01-42-39\"\n\n    return {\n        \"namespace\": namespace\n    }\n\nclass RetrieverRequest(BaseModel):\n    namespace: str\n\n# @app.post(\"/setRetriever\")\n# async def set_retriever(req: RetrieverRequest):\n#     docsearch = PineconeVectorStore.from_existing_index(\n#         index_name='alphawell',\n#         embedding=embeddings,\n#         namespace=req.namespace\n#     )\n\n#     retriever = docsearch.as_retriever(\n#         search_type=\"similarity\",\n#         search_kwargs={\"k\": 10}\n#     )\n\n#     global current_retriever\n#     current_retriever = retriever\n\n#     return {\"message\": f\"Retriever set for the namespace '{req.namespace}'\"}\n\n# class QueryRequest(BaseModel):\n#     request: str\n\n# # Agentic AI\n# @app.post(\"/alpha_bot29\")\n# async def respond(data: QueryRequest):\n#     answer = askDeepSeek(query = data.request)\n#     return {\"answer\": answer}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T20:18:44.792224Z","iopub.execute_input":"2025-08-26T20:18:44.792853Z","iopub.status.idle":"2025-08-26T20:18:44.968492Z","shell.execute_reply.started":"2025-08-26T20:18:44.792821Z","shell.execute_reply":"2025-08-26T20:18:44.967785Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\nngrok_label = \"NGROK_AUTH_TOKEN\"\n\nngrok_token = UserSecretsClient().get_secret(ngrok_label)\n\n!ngrok authtoken $ngrok_token\n\nngrok_tunnel = ngrok.connect(8000)\nprint(\"Public URL\", ngrok_tunnel.public_url)\nnest_asyncio.apply()\nuvicorn.run(app, port=8000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T20:18:44.969290Z","iopub.execute_input":"2025-08-26T20:18:44.969548Z","iopub.status.idle":"2025-08-26T20:35:00.913350Z","shell.execute_reply.started":"2025-08-26T20:18:44.969530Z","shell.execute_reply":"2025-08-26T20:35:00.912655Z"}},"outputs":[{"name":"stdout","text":"Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n","output_type":"stream"},{"name":"stderr","text":"INFO:     Started server process [294]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n","output_type":"stream"},{"name":"stdout","text":"Public URL https://d06023c381d6.ngrok-free.app\n","output_type":"stream"},{"name":"stderr","text":"INFO:     Shutting down\nINFO:     Waiting for application shutdown.\nINFO:     Application shutdown complete.\nINFO:     Finished server process [294]\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n    ColabKernelApp.launch_instance()\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1921, in _run_once\n    handle = self._ready.popleft()\n             ^^^^^^^^^^^^^^^^^^^^^\nIndexError: pop from an empty deque\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}